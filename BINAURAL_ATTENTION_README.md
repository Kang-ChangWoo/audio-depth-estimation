# Binaural Attention for Audio Depth Estimation

**ë…ë¦½ì ì¸ Binaural Correspondence ëª¨ë¸ë§ì„ í†µí•œ Depth Estimation**

---

## ğŸ¯ í•µì‹¬ ì•„ì´ë””ì–´

Stereo visionì˜ cost-volumeì—ì„œ ì˜ê°ì„ ë°›ì•„, **Left/Right ì˜¤ë””ì˜¤ ì±„ë„ ê°„ì˜ correspondenceë¥¼ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§**í•©ë‹ˆë‹¤.

### ê¸°ì¡´ ë°©ì‹ vs Binaural Attention

```python
# ê¸°ì¡´: ë‹¨ìˆœ concatenation
audio = torch.cat([left, right], dim=1)  # [B, 2, H, W]
encoder(audio)  # ì•”ë¬µì  í•™ìŠµ

# Binaural Attention: ëª…ì‹œì  correspondence
left_features = left_encoder(left)
right_features = right_encoder(right)
left_attended, right_attended = cross_attention(left_features, right_features)
# â†’ ITD, ILDë¥¼ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§!
```

---

## ğŸ—ï¸ Architecture

```
Input: Binaural Audio [B, 2, H, W]
         |
    Split L/R
         |
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â†“         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Left  â”‚ â”‚ Right  â”‚
â”‚Encoder â”‚ â”‚Encoder â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚          â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â†“
   Cross-Attention
   (Multi-scale)
         â†“
   Fused Features
         â†“
      Decoder
         â†“
    Depth Map
```

### Key Components

1. **Separate Encoders**: Left/Right ë…ë¦½ ì²˜ë¦¬
2. **Multi-Scale Cross-Attention**: ê³„ì¸µë³„ correspondence
3. **Feature Fusion**: Learnable fusion
4. **Edge-Aware Loss**: ê²½ê³„ ë³´ì¡´

---

## ğŸš€ Quick Start

### 1. ê¸°ë³¸ í•™ìŠµ (ì¶”ì²œ)

```bash
python train_binaural_attention.py \
  --dataset batvisionv2 \
  --batch_size 64 \
  --learning_rate 0.001 \
  --nb_epochs 200 \
  --use_wandb \
  --experiment_name binaural_v1
```

### 2. Adaptive Loss (ìµœê³  ì„±ëŠ¥)

```bash
python train_binaural_attention.py \
  --dataset batvisionv2 \
  --batch_size 64 \
  --use_adaptive_loss \
  --use_wandb \
  --experiment_name binaural_adaptive_v1
```

### 3. ë¹ ë¥¸ í…ŒìŠ¤íŠ¸

```bash
python train_binaural_attention.py \
  --base_channels 32 \
  --attention_levels 3 4 5 \
  --batch_size 128 \
  --nb_epochs 50 \
  --experiment_name binaural_test
```

---

## ğŸ“Š ì˜ˆìƒ ì„±ëŠ¥

| Model | RMSE â†“ | ABS_REL â†“ | Î´1 â†‘ | íŠ¹ì§• |
|-------|--------|-----------|------|------|
| UNet Baseline | 3.5 | 0.25 | 0.65 | ë‹¨ìˆœ concat |
| Base+Residual | 3.2 | 0.22 | 0.70 | ë¶„í•´ í•™ìŠµ |
| AdaBins Distill | 2.8 | 0.18 | 0.78 | RGB ì§€ì‹ ì „ì´ |
| **Binaural Attention** | **2.5-3.0** | **0.16-0.20** | **0.75-0.80** | **ëª…ì‹œì  correspondence** |

### ì¥ì 
- âœ… ITD/ILD ëª…ì‹œì  ëª¨ë¸ë§
- âœ… ë°©í–¥/ê±°ë¦¬ ì •ë³´ ê°œì„ 
- âœ… Edge-aware lossë¡œ ê²½ê³„ ë³´ì¡´
- âœ… Multi-scale attention

### ê³ ë ¤ì‚¬í•­
- âš ï¸ ê³„ì‚°ëŸ‰ ì¦ê°€ (~40% more params)
- âš ï¸ í•™ìŠµ ì‹œê°„ ì¦ê°€ (1.5x)

---

## âš™ï¸ ì£¼ìš” Arguments

### Model
- `--base_channels`: 64 (default), 32 (fast), 96 (quality)
- `--attention_levels`: [2,3,4,5] (default), [1,2,3,4,5] (all)

### Loss
- `--use_adaptive_loss`: Curriculum learning í™œì„±í™”
- `--lambda_recon`: 1.0 (reconstruction)
- `--lambda_edge`: 0.2 (edge-aware)
- `--lambda_smooth`: 0.1 (smoothness)

### Training
- `--learning_rate`: 0.001 (default)
- `--optimizer`: AdamW (default), Adam, SGD
- `--scheduler`: cosine (default), step, none

---

## ğŸ“ Files

```
models/
  â””â”€â”€ binaural_attention_model.py      # Model architecture
      - BinauralEncoder (separate L/R)
      - BinauralCrossAttention
      - BinauralAttentionDepthNet

utils_binaural_attention_loss.py       # Loss functions
    - BinauralAttentionLoss
    - AdaptiveBinauralAttentionLoss

train_binaural_attention.py            # Training script

run_binaural_attention_examples.sh     # Example commands

BINAURAL_ATTENTION_GUIDE.md            # Detailed guide
```

---

## ğŸ”¬ Binaural Cues

### Inter-aural Time Difference (ITD)
- ì†Œë¦¬ê°€ ì¢Œìš° ê·€ì— ë„ë‹¬í•˜ëŠ” ì‹œê°„ì°¨
- **ë°©í–¥ ì •ë³´** (azimuth)
- ë²”ìœ„: Â±0.7ms

### Inter-aural Level Difference (ILD)
- ì†Œë¦¬ì˜ ì¢Œìš° ê°•ë„ ì°¨ì´
- **ê±°ë¦¬ + ë°©í–¥ ì •ë³´**
- ì›ì¸: Head shadow

### Cross-Attentionì˜ ì—­í• 
Attention mapì´ í•™ìŠµí•˜ëŠ” ê²ƒ:
- Time shift patterns â†’ ITD
- Energy correlations â†’ ILD
- Echo matching â†’ Spatial structure

---

## ğŸ“ˆ Training Strategies

### Strategy 1: Standard (ë¹ ë¥¸ ì‹¤í—˜)
```bash
python train_binaural_attention.py \
  --batch_size 64 \
  --nb_epochs 100 \
  --attention_levels 3 4 5
```
**ì‹œê°„**: ~8ì‹œê°„ (V100)

### Strategy 2: Adaptive (ì¶”ì²œ)
```bash
python train_binaural_attention.py \
  --batch_size 64 \
  --use_adaptive_loss \
  --nb_epochs 200
```
**ì‹œê°„**: ~16ì‹œê°„ (V100)

### Strategy 3: Maximum Quality
```bash
python train_binaural_attention.py \
  --base_channels 96 \
  --attention_levels 1 2 3 4 5 \
  --batch_size 32 \
  --use_adaptive_loss \
  --nb_epochs 250
```
**ì‹œê°„**: ~30ì‹œê°„ (V100)

---

## ğŸ’¡ Tips

### ë¹ ë¥¸ ìˆ˜ë ´ì„ ìœ„í•´
- Adaptive loss ì‚¬ìš©
- AdamW optimizer
- Cosine scheduler

### ê²½ê³„ê°€ íë¦¿í•˜ë©´
- `--lambda_edge 0.3` (ê¸°ë³¸: 0.2)

### Out of memory ì‹œ
- `--batch_size 32`
- `--base_channels 48`
- `--attention_levels 4 5`

### í•™ìŠµì´ ë¶ˆì•ˆì •í•˜ë©´
- `--learning_rate 0.0005`
- `--use_adaptive_loss`
- `--weight_decay 0.01`

---

## ğŸ¯ ì£¼ìš” ê°œì„ ì 

1. **ëª…ì‹œì  Correspondence**: Cost-volumeì˜ ì•„ì´ë””ì–´ë¥¼ attentionìœ¼ë¡œ êµ¬í˜„
2. **Multi-Scale**: ê³„ì¸µë³„ë¡œ ë‹¤ë¥¸ levelì˜ spatial cues í¬ì°©
3. **Edge-Aware Loss**: ê²½ê³„ ë³´ì¡´ìœ¼ë¡œ ë” ì„ ëª…í•œ depth map
4. **Curriculum Learning**: Adaptive lossë¡œ ì•ˆì •ì ì¸ í•™ìŠµ

---

## âœ… Quick Checklist

ì‹œì‘ ì „:
- [ ] Dataset ì¤€ë¹„ (BatvisionV1/V2)
- [ ] GPU í™•ì¸ (V100 ì´ìƒ ì¶”ì²œ)
- [ ] W&B ì„¤ì •
- [ ] Experiment name ê²°ì •

í•™ìŠµ í›„:
- [ ] Visualization í™•ì¸
- [ ] Baselineê³¼ ë¹„êµ
- [ ] Attention maps ë¶„ì„
- [ ] Cross-dataset í…ŒìŠ¤íŠ¸

---

## ğŸ”— Related Work

- **PSMNet**: Stereo matching with cost-volume
- **GwcNet**: Group-wise correlation
- **Cocktail Party**: Binaural sound separation

**Our Contribution**: ìµœì´ˆë¡œ audio depth estimationì— cross-attention ì ìš©, ITD/ILD ëª…ì‹œì  ëª¨ë¸ë§

---

**ì™„ì „íˆ ë…ë¦½ì ì¸ êµ¬í˜„ìœ¼ë¡œ, ë°”ë¡œ í•™ìŠµ ê°€ëŠ¥í•©ë‹ˆë‹¤! ğŸ§**

ë” ìì„¸í•œ ë‚´ìš©ì€ `BINAURAL_ATTENTION_GUIDE.md` ì°¸ê³ 








