# RGB Depth Model - Quick Summary

## μƒμ„±λ νμΌλ“¤

### 1. λ¨λΈ νμΌ
**`models/rgb_depth_model.py`** (μƒλ΅ μƒμ„±)
- RGB μ…λ ¥ (3μ±„λ„)μ„ μ„ν• Depth Estimation λ¨λΈ
- U-Net κΈ°λ° encoder-decoder κµ¬μ΅°
- Binaural attention λ¨λΈκ³Ό **feature ν¬κΈ° νΈν™**
- `return_features=True` μµμ…μΌλ΅ distillationμ„ μ„ν• μ¤‘κ°„ feature μ¶”μ¶ κ°€λ¥

```python
# μ‚¬μ© μμ 
from models.rgb_depth_model import create_rgb_depth_model
model = create_rgb_depth_model(base_channels=64)

# μΌλ° forward
depth = model(rgb_image)

# Distillationμ„ μ„ν• feature μ¶”μ¶
depth, features = model(rgb_image, return_features=True)
# features = {'x1', 'x2', 'x3', 'x4', 'x5', 'd1', 'd2', 'd3', 'd4'}
```

### 2. ν•™μµ μ¤ν¬λ¦½νΈ
**`train_rgb_depth.py`** (μƒλ΅ μƒμ„±)
- `train_binaural_attention.py`μ™€ μ μ‚¬ν• κµ¬μ΅°
- BatvisionV1/V2 λ°μ΄ν„°μ…‹ μ§€μ›
- W&B λ΅κΉ… μ§€μ›
- Checkpoint μ €μ¥/λ΅λ”© κΈ°λ¥

```bash
# κΈ°λ³Έ ν•™μµ
python train_rgb_depth.py --dataset batvisionv2 --batch_size 64 --use_wandb

# Teacher λ¨λΈ ν•™μµ (distillationμ©)
python train_rgb_depth.py \
    --dataset batvisionv2 \
    --base_channels 64 \
    --nb_epochs 200 \
    --experiment_name rgb_teacher_for_kd
```

### 3. μ‹¤ν–‰ μμ  μ¤ν¬λ¦½νΈ
**`run_rgb_depth_examples.sh`** (μƒλ΅ μƒμ„±)
- 8κ°€μ§€ μ‚¬μ „ μ„¤μ •λ ν•™μµ μμ 
- Interactive λ©”λ‰΄ λλ” μ§μ ‘ μ‹¤ν–‰ κ°€λ¥

```bash
# Interactive λ¨λ“
bash run_rgb_depth_examples.sh

# νΉμ • μμ  μ‹¤ν–‰
bash run_rgb_depth_examples.sh 6  # Teacher model ν•™μµ
```

### 4. νΈν™μ„± κ²€μ¦ μ¤ν¬λ¦½νΈ
**`verify_feature_compatibility.py`** (μƒλ΅ μƒμ„±)
- RGB λ¨λΈκ³Ό Binaural λ¨λΈμ feature μ°¨μ› νΈν™μ„± κ²€μ¦
- Distillation μ¤€λΉ„ μƒνƒ ν™•μΈ

```bash
python verify_feature_compatibility.py
```

### 5. λ¬Έμ„
**`RGB_DEPTH_README.md`** (μƒλ΅ μƒμ„±)
- μƒμ„Έν• μ‚¬μ© μ„¤λ…μ„
- μ•„ν‚¤ν…μ² λΉ„κµ
- Distillation κ°€μ΄λ“
- μμ  μ½”λ“

---

## ν•µμ‹¬ μ°¨μ΄μ : RGB vs. Binaural Audio

| νΉμ„± | RGB Model | Binaural Audio Model |
|------|-----------|----------------------|
| **μ…λ ¥** | 3 channels (RGB) | 2 channels (Stereo Audio) |
| **Encoder** | Single encoder | Dual encoder (Left/Right) |
| **νΉμ λ¨λ“** | μ—†μ | Cross-attention between L/R |
| **νλΌλ―Έν„° μ** | ~20M (base=64) | ~40M (base=64) |
| **Feature ν¬κΈ°** | β… νΈν™ | β… νΈν™ |

---

## Feature νΈν™μ„± (Distillationμ„ μ„ν•΄ μ¤‘μ”!)

λ‘ λ¨λΈ λ¨λ‘ λ™μΌν• feature μ°¨μ›μ„ μƒμ„±:

```
Level x1: [B, 64, 256, 256]     β† RGB encoder == Audio fusion output
Level x2: [B, 128, 128, 128]    β† RGB encoder == Audio fusion output
Level x3: [B, 256, 64, 64]      β† RGB encoder == Audio fusion output
Level x4: [B, 512, 32, 32]      β† RGB encoder == Audio fusion output
Level x5: [B, 512, 16, 16]      β† RGB encoder == Audio fusion output
```

μ΄ νΈν™μ„± λ•λ¶„μ—:
- **Feature-level distillation** κ°€λ¥
- RGB teacher β†’ Audio student μ§€μ‹ μ „λ‹¬
- μ¤‘κ°„ layerμ representationμ„ μ§μ ‘ λ§¤μΉ­

---

## λ‹¤μ λ‹¨κ³„: Distillation κµ¬ν„

### 1λ‹¨κ³„: RGB Teacher ν•™μµ

```bash
bash run_rgb_depth_examples.sh 6
```

### 2λ‹¨κ³„: Distillation μ½”λ“ μ‘μ„±

`train_distillation.py` μƒμ„± (pseudo-code):

```python
# Teacher (RGB)
teacher = create_rgb_depth_model(base_channels=64)
teacher.load_state_dict(torch.load('checkpoints/rgb_teacher/best_model.pth'))
teacher.eval()

# Student (Audio)
student = create_binaural_attention_model(base_channels=64)

# Training loop
for audio, rgb, depth_gt in dataloader:
    # Teacher prediction
    with torch.no_grad():
        depth_teacher, feats_teacher = teacher(rgb, return_features=True)
    
    # Student prediction (audioλ§ μ‚¬μ©)
    depth_student, feats_student = student(audio, return_features=True)
    
    # Losses
    loss_task = criterion(depth_student, depth_gt)  # Ground truth
    loss_kd = F.mse_loss(depth_student, depth_teacher)  # Depth distillation
    
    # Feature matching
    loss_feat = 0
    for level in ['x1', 'x2', 'x3', 'x4', 'x5']:
        loss_feat += F.mse_loss(feats_student[level], feats_teacher[level])
    
    # Total loss
    loss = loss_task + Ξ»_kd * loss_kd + Ξ»_feat * loss_feat
    loss.backward()
    optimizer.step()
```

### 3λ‹¨κ³„: Audio λ¨λΈ μμ •

`binaural_attention_model.py`μ— feature λ°ν™ κΈ°λ¥ μ¶”κ°€:

```python
def forward(self, x, return_features=False):
    # ... existing code ...
    
    if return_features:
        features = {
            'x1': left_feats['x1'],  # After fusion
            'x2': left_feats['x2'],
            'x3': left_feats['x3'],
            'x4': left_feats['x4'],
            'x5': left_feats['x5']
        }
        return depth, features
    return depth
```

---

## μ‹¤ν— μ μ•

### Baseline
1. **RGB only** (upper bound)
2. **Audio only** (baseline)

### Distillation Experiments
3. **Audio + KD (depth)**: Depth μμΈ΅λ§ distillation
4. **Audio + KD (depth + features)**: Depth + intermediate features distillation
5. **Audio + KD (adaptive)**: ν•™μµ μ§„ν–‰μ— λ”°λΌ distillation weight μ΅°μ •

### μμƒ κ²°κ³Ό
```
RGB only:              RMSE = X (best)
Audio only:            RMSE = Y
Audio + KD (depth):    RMSE = Y - Ξ΄1
Audio + KD (full):     RMSE = Y - Ξ΄2 (Ξ΄2 > Ξ΄1)
```

---

## μ²΄ν¬λ¦¬μ¤νΈ

- [x] RGB λ¨λΈ κµ¬ν„ (`rgb_depth_model.py`)
- [x] RGB ν•™μµ μ¤ν¬λ¦½νΈ (`train_rgb_depth.py`)
- [x] μ‹¤ν–‰ μμ  μ¤ν¬λ¦½νΈ (`run_rgb_depth_examples.sh`)
- [x] Feature νΈν™μ„± κ²€μ¦ (`verify_feature_compatibility.py`)
- [x] λ¬Έμ„ μ‘μ„± (`RGB_DEPTH_README.md`, `RGB_SUMMARY.md`)
- [ ] RGB teacher λ¨λΈ ν•™μµ
- [ ] Audio λ¨λΈμ— `return_features` μ¶”κ°€
- [ ] Distillation ν•™μµ μ¤ν¬λ¦½νΈ μ‘μ„±
- [ ] Distillation μ‹¤ν— λ° ν‰κ°€

---

## λΉ λ¥Έ μ‹μ‘

```bash
# 1. Feature νΈν™μ„± ν™•μΈ
python verify_feature_compatibility.py

# 2. RGB teacher λ¨λΈ ν•™μµ μ‹μ‘
bash run_rgb_depth_examples.sh 6

# 3. ν•™μµ λ¨λ‹ν„°λ§ (W&B μ‚¬μ© μ‹)
# https://wandb.ai/your-project/batvision-rgb-depth

# 4. Best checkpoint ν™•μΈ
ls -lh checkpoints/rgb_teacher_for_kd/best_model.pth
```

---

**λ¨λ“  νμΌμ΄ μ¤€λΉ„λμ—μµλ‹λ‹¤! π‰**

μ΄μ  RGB teacher λ¨λΈμ„ ν•™μµν•κ³ , distillationμ„ ν†µν•΄ audio λ¨λΈμ μ„±λ¥μ„ ν–¥μƒμ‹ν‚¬ μ μμµλ‹λ‹¤.






