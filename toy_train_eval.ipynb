{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/evtAnything/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA RTX A5000\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import torchvision.models as models\n",
    "import timm\n",
    "\n",
    "# Import existing utils\n",
    "from utils_criterion import compute_errors\n",
    "from models.unetbaseline_model import define_G  # â† U-Net ì§€ì›!\n",
    "\n",
    "# GPU setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = [\n",
    "    \"2ndFloorLuxembourg\",\n",
    "    \"3rd_Floor_Luxembourg\",\n",
    "    \"Attic\",\n",
    "    \"Outdoor_Cobblestone_Path\",\n",
    "    \"Salle_Chevalier\",\n",
    "    \"Salle_des_Colonnes\",\n",
    "    \"V119_Cake_Corridors\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Dataset: ALL LOCATIONS (7 sequences)\n",
      "  Model: resnet\n",
      "  Epochs: 50, LR: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# ========== Configuration ==========\n",
    "\n",
    "# Data settings\n",
    "ROOT_DIR = '/root/dev/data/dataset/Batvision/BatvisionV2/'\n",
    "USE_ALL_LOCATIONS = True  # True: ì „ì²´ ì‹œí€€ìŠ¤, False: ë‹¨ì¼ ì‹œí€€ìŠ¤\n",
    "LOCATION = 'Salle_des_Colonnes'\n",
    "MAX_DEPTH = 30.0\n",
    "IMG_SIZE = 256\n",
    "\n",
    "# Train/Eval split\n",
    "USE_TRAIN_AS_EVAL = False  # True: trainìœ¼ë¡œ í‰ê°€, False: val ì‚¬ìš©\n",
    "\n",
    "# Model settings\n",
    "MODEL_TYPE = 'resnet'  # 'resnet' or 'unet_256'\n",
    "PRETRAINED = True\n",
    "\n",
    "# Training settings\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# Logging\n",
    "PRINT_EVERY = 5\n",
    "VIS_SAMPLES = 3\n",
    "\n",
    "print(\"Configuration:\")\n",
    "if USE_ALL_LOCATIONS:\n",
    "    print(f\"  Dataset: ALL LOCATIONS ({len(locations)} sequences)\")\n",
    "else:\n",
    "    print(f\"  Dataset: {LOCATION}\")\n",
    "print(f\"  Model: {MODEL_TYPE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}, LR: {LEARNING_RATE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽµ Depth Any Audio: Cross-Modal Distillation\n",
    "\n",
    "Inspired by **Depth AnyEvent** (ICCV 2025), we implement cross-modal distillation:\n",
    "- **Teacher**: Depth Anything V2 (RGB â†’ Depth) - Vision Foundation Model\n",
    "- **Student**: Audio U-Net (Binaural Audio â†’ Depth) - Event-like sensor\n",
    "- **Strategy**: Teacher generates proxy depth labels from RGB images to supervise the audio-based student\n",
    "\n",
    "This approach eliminates the need for expensive depth annotations!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration for Depth Any Audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Depth Any Audio Configuration\n",
      "================================================================================\n",
      "Mode: ðŸŽµ Feature-Level Knowledge Distillation\n",
      "  Teacher: vitl_feature (ViT-L, frozen: True)\n",
      "    Feature layers: [6, 12, 18, 24]\n",
      "    Feature KD loss: cosine (Î»=2.0)\n",
      "  Student: unet_256 (base_channels: 64)\n",
      "  Loss: combined\n",
      "    Î»_L1: 0.5, Î»_SIlog: 0.5, SIlog_Î»: 0.85\n",
      "  Supervision: Feature KD + GT (Î»_feature=2.0, Î»_GT=1.0)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ========== Depth Any Audio Configuration ==========\n",
    "\n",
    "# Training mode\n",
    "# OPTIONS:\n",
    "#   - USE_DISTILLATION=True: Cross-modal distillation (Teacher RGB + Student Audio)\n",
    "#   - USE_DISTILLATION=False: Supervised learning (Student Audio + GT only)\n",
    "USE_DISTILLATION = True  # True: Depth Any Audio (distillation), False: Standard supervised\n",
    "\n",
    "# Teacher model settings\n",
    "# OPTIONS:\n",
    "#   - 'depthanything_v2_vitl': Depth Anything V2 (depth prediction level KD)\n",
    "#   - 'vitl_feature': ViT-L (feature level KD from spectrogram)\n",
    "TEACHER_MODEL = 'vitl_feature'  # 'depthanything_v2_vitl' or 'vitl_feature'\n",
    "TEACHER_ENCODER = 'vitl'  # vits, vitb, vitl\n",
    "FREEZE_TEACHER = True  # Always freeze teacher\n",
    "\n",
    "# Feature-level KD settings (for ViT-L teacher)\n",
    "USE_FEATURE_KD = True  # True: Feature-level KD, False: Prediction-level KD\n",
    "FEATURE_KD_LAYERS = [6, 12, 18, 24]  # Which transformer blocks to use for KD (0-indexed, total 24 for ViT-L)\n",
    "FEATURE_KD_LAMBDA = 2.0  # Weight for feature-level KD loss (increased for better KD effect)\n",
    "FEATURE_KD_LOSS_TYPE = 'cosine'  # 'mse' or 'cosine' for feature matching (cosine is better for normalized features)\n",
    "\n",
    "# Student model settings (Audio U-Net)\n",
    "STUDENT_MODEL = 'unet_256'  # Audio-based depth estimator\n",
    "STUDENT_BASE_CHANNELS = 64  # 64 for standard, 32 for lightweight\n",
    "\n",
    "# Distillation loss settings\n",
    "DISTILLATION_LOSS = 'combined'  # 'l1', 'silog', 'combined'\n",
    "LAMBDA_L1 = 0.5\n",
    "LAMBDA_SILOG = 0.5\n",
    "SILOG_LAMBDA = 0.85\n",
    "\n",
    "# Ground truth supervision settings\n",
    "USE_GT_SUPERVISION = True  # Use GT depth for supervision (always True for fair comparison)\n",
    "if USE_DISTILLATION:\n",
    "    if USE_FEATURE_KD and TEACHER_MODEL == 'vitl_feature':\n",
    "        # Feature-level KD: Only feature KD + GT supervision\n",
    "        LAMBDA_DISTILL = 0.0  # No prediction-level distillation\n",
    "        LAMBDA_GT = 1.0  # Weight for GT supervision loss\n",
    "    else:\n",
    "        # Prediction-level KD: Teacher pseudo-label + GT supervision\n",
    "        LAMBDA_DISTILL = 0.5  # Weight for distillation loss (teacher pseudo-label)\n",
    "        LAMBDA_GT = 0.5  # Weight for GT supervision loss\n",
    "else:\n",
    "    # Supervised learning mode: Only GT supervision\n",
    "    LAMBDA_DISTILL = 0.0\n",
    "    LAMBDA_GT = 1.0\n",
    "\n",
    "# Data settings for distillation\n",
    "USE_RGB_TEACHER = True  # Use RGB images for teacher (only for depthanything_v2)\n",
    "USE_AUDIO_STUDENT = True  # Use audio for student\n",
    "\n",
    "# Training settings for distillation\n",
    "DISTILL_BATCH_SIZE = 2  # Further reduced batch size for ViT-L (memory intensive)\n",
    "DISTILL_GRAD_ACCUM = 8  # Gradient accumulation steps (effective batch = 2 * 8 = 16)\n",
    "DISTILL_EPOCHS = 100\n",
    "DISTILL_LR = 1e-4\n",
    "DISTILL_WEIGHT_DECAY = 1e-4\n",
    "USE_MIXED_PRECISION = True  # Use mixed precision to save memory\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Depth Any Audio Configuration\")\n",
    "print(\"=\" * 80)\n",
    "if USE_DISTILLATION:\n",
    "    if USE_FEATURE_KD and TEACHER_MODEL == 'vitl_feature':\n",
    "        print(\"Mode: ðŸŽµ Feature-Level Knowledge Distillation\")\n",
    "        print(f\"  Teacher: {TEACHER_MODEL} (ViT-L, frozen: {FREEZE_TEACHER})\")\n",
    "        print(f\"    Feature layers: {FEATURE_KD_LAYERS}\")\n",
    "        print(f\"    Feature KD loss: {FEATURE_KD_LOSS_TYPE} (Î»={FEATURE_KD_LAMBDA})\")\n",
    "        print(f\"  Student: {STUDENT_MODEL} (base_channels: {STUDENT_BASE_CHANNELS})\")\n",
    "        print(f\"  Loss: {DISTILLATION_LOSS}\")\n",
    "        print(f\"    Î»_L1: {LAMBDA_L1}, Î»_SIlog: {LAMBDA_SILOG}, SIlog_Î»: {SILOG_LAMBDA}\")\n",
    "        print(f\"  Supervision: Feature KD + GT (Î»_feature={FEATURE_KD_LAMBDA}, Î»_GT={LAMBDA_GT})\")\n",
    "    else:\n",
    "        print(\"Mode: ðŸŽµ Cross-Modal Distillation (Prediction-Level KD)\")\n",
    "        print(f\"  Teacher: {TEACHER_MODEL} (frozen: {FREEZE_TEACHER})\")\n",
    "        print(f\"  Student: {STUDENT_MODEL} (base_channels: {STUDENT_BASE_CHANNELS})\")\n",
    "        print(f\"  Loss: {DISTILLATION_LOSS}\")\n",
    "        print(f\"    Î»_L1: {LAMBDA_L1}, Î»_SIlog: {LAMBDA_SILOG}, SIlog_Î»: {SILOG_LAMBDA}\")\n",
    "        print(f\"  Supervision: Î»_distill={LAMBDA_DISTILL}, Î»_GT={LAMBDA_GT}\")\n",
    "else:\n",
    "    print(\"Mode: ðŸ“š Supervised Learning (Student Audio + GT only)\")\n",
    "    print(f\"  Student: {STUDENT_MODEL} (base_channels: {STUDENT_BASE_CHANNELS})\")\n",
    "    print(f\"  Loss: {DISTILLATION_LOSS}\")\n",
    "    print(f\"    Î»_L1: {LAMBDA_L1}, Î»_SIlog: {LAMBDA_SILOG}, SIlog_Î»: {SILOG_LAMBDA}\")\n",
    "    print(f\"  Supervision: GT only (Î»_GT={LAMBDA_GT})\")\n",
    "    print(\"  âš ï¸  Teacher model will NOT be loaded or used\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Testing DepthAnyAudioDataset\n",
      "================================================================================\n",
      "Loaded 431 samples from 2ndFloorLuxembourg (train)\n",
      "Loaded 290 samples from 3rd_Floor_Luxembourg (train)\n",
      "Loaded 37 samples from Attic (train)\n",
      "Loaded 377 samples from Outdoor_Cobblestone_Path (train)\n",
      "Loaded 116 samples from Salle_Chevalier (train)\n",
      "Loaded 240 samples from Salle_des_Colonnes (train)\n",
      "Loaded 420 samples from V119_Cake_Corridors (train)\n",
      "âœ… Total: 1911 samples from 7 location(s)\n",
      "\n",
      "Sample 0:\n",
      "  Image shape: torch.Size([3, 256, 256]), range: [0.000, 1.000]\n",
      "  Audio shape: torch.Size([2, 256, 256]), range: [0.063, 0.998]\n",
      "  Depth shape: torch.Size([1, 256, 256]), range: [0.000, 11.929]\n",
      "  Filename: camera_0.jpeg\n",
      "âœ… Dataset test passed!\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "class DepthAnyAudioDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Depth Any Audio: Returns RGB image, Audio spectrogram, and Depth\n",
    "    - RGB: For teacher model (Depth Anything V2)\n",
    "    - Audio: For student model (Audio U-Net)\n",
    "    - Depth: Ground truth (optional, for validation)\n",
    "    \n",
    "    Supports multiple locations (similar to BatvisionV2_Dataset.py)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, locations=None, split='train', max_depth=30.0, img_size=256, \n",
    "                 audio_format='spectrogram', use_gt_depth=True, location_blacklist=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir: Root directory of the dataset\n",
    "            locations: List of locations or single location string. If None, uses all valid locations.\n",
    "            split: 'train' or 'val'\n",
    "            max_depth: Maximum depth in meters\n",
    "            img_size: Image size for resizing\n",
    "            audio_format: 'spectrogram' or 'waveform'\n",
    "            use_gt_depth: Whether to load ground truth depth\n",
    "            location_blacklist: List of locations to exclude\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.max_depth = max_depth\n",
    "        self.img_size = img_size\n",
    "        self.audio_format = audio_format\n",
    "        self.use_gt_depth = use_gt_depth\n",
    "        \n",
    "        # Handle locations\n",
    "        if locations is None:\n",
    "            # Use all valid directories in root_dir\n",
    "            location_list = [item for item in os.listdir(root_dir) \n",
    "                           if os.path.isdir(os.path.join(root_dir, item))\n",
    "                           and not item.startswith('.')\n",
    "                           and not item.startswith('__')\n",
    "                           and not item.endswith('_unzipped')]\n",
    "        elif isinstance(locations, str):\n",
    "            # Single location\n",
    "            location_list = [locations]\n",
    "        else:\n",
    "            # Multiple locations\n",
    "            location_list = locations\n",
    "        \n",
    "        # Apply blacklist\n",
    "        if location_blacklist:\n",
    "            location_list = [loc for loc in location_list if loc not in location_blacklist]\n",
    "        \n",
    "        # Load CSVs from all valid locations\n",
    "        location_csv_paths = []\n",
    "        for location in location_list:\n",
    "            csv_path = os.path.join(root_dir, location, f'{split}.csv')\n",
    "            if os.path.exists(csv_path):\n",
    "                location_csv_paths.append(csv_path)\n",
    "            else:\n",
    "                print(f\"âš ï¸  Warning: {csv_path} not found, skipping location {location}\")\n",
    "        \n",
    "        if len(location_csv_paths) == 0:\n",
    "            raise ValueError(f\"No valid locations found with {split}.csv in {root_dir}. \"\n",
    "                           f\"Checked {len(location_list)} directories: {location_list[:5]}...\")\n",
    "        \n",
    "        # Load and concatenate all CSVs\n",
    "        self.instances = []\n",
    "        for csv_path in location_csv_paths:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            location_name = os.path.basename(os.path.dirname(csv_path))\n",
    "            print(f\"Loaded {len(df)} samples from {location_name} ({split})\")\n",
    "            self.instances.append(df)\n",
    "        \n",
    "        self.data = pd.concat(self.instances, ignore_index=True)\n",
    "        print(f\"âœ… Total: {len(self.data)} samples from {len(location_csv_paths)} location(s)\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # === Load RGB Image ===\n",
    "        # Fix: Use 'camera path' and 'camera file name' instead of 'image path' and 'image file name'\n",
    "        img_path = os.path.join(self.root_dir, row['camera path'], row['camera file name'])\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            raise RuntimeError(f\"Could not load image file {img_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (self.img_size, self.img_size))\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)  # [3, H, W]\n",
    "        \n",
    "        # === Load Audio (Binaural) ===\n",
    "        # Fix: Use torchaudio.load() with fallback methods instead of np.load()\n",
    "        audio_path = os.path.join(self.root_dir, row['audio path'], row['audio file name'])\n",
    "        waveform, sr = self._load_audio(audio_path)\n",
    "        \n",
    "        # Cut audio to fit max depth (same as BatvisionV2_Dataset.py)\n",
    "        if self.max_depth:\n",
    "            cut = int((2 * self.max_depth / 340) * sr)\n",
    "            waveform = waveform[:, :cut]\n",
    "        \n",
    "        # Convert to spectrogram\n",
    "        if self.audio_format == 'spectrogram':\n",
    "            # Use parameters from BatvisionV2_Dataset.py for cut audio\n",
    "            win_length = 64\n",
    "            n_fft = 512\n",
    "            hop_length = 64 // 4\n",
    "            \n",
    "            spec_transform = T.Spectrogram(\n",
    "                n_fft=n_fft,\n",
    "                win_length=win_length,\n",
    "                hop_length=hop_length,\n",
    "                power=1.0\n",
    "            )\n",
    "            audio = spec_transform(waveform)  # [2, freq, time]\n",
    "            \n",
    "            # Apply log scale (same as BatvisionV2_Dataset.py)\n",
    "            audio = torch.log(audio + 1e-8)\n",
    "            \n",
    "            # Min-max normalize each channel independently to [0, 1]\n",
    "            for c in range(audio.shape[0]):\n",
    "                spec_min = audio[c].min()\n",
    "                spec_max = audio[c].max()\n",
    "                if spec_max > spec_min:\n",
    "                    audio[c] = (audio[c] - spec_min) / (spec_max - spec_min)\n",
    "                else:\n",
    "                    audio[c] = torch.zeros_like(audio[c])\n",
    "            \n",
    "            # Resize to match image size\n",
    "            audio = F.interpolate(audio.unsqueeze(0), size=(self.img_size, self.img_size), \n",
    "                                mode='bilinear', align_corners=False).squeeze(0)\n",
    "        else:\n",
    "            audio = waveform\n",
    "        \n",
    "        # === Load Depth (Ground Truth) ===\n",
    "        if self.use_gt_depth:\n",
    "            depth_path = os.path.join(self.root_dir, row['depth path'], row['depth file name'])\n",
    "            depth = np.load(depth_path).astype(np.float32)\n",
    "            depth = depth / 1000.0  # mm to m\n",
    "            if self.max_depth:\n",
    "                depth[depth > self.max_depth] = self.max_depth\n",
    "            depth[depth < 0] = 0\n",
    "            depth = cv2.resize(depth, (self.img_size, self.img_size), \n",
    "                             interpolation=cv2.INTER_NEAREST)\n",
    "            depth = torch.from_numpy(depth).unsqueeze(0)  # [1, H, W]\n",
    "        else:\n",
    "            depth = torch.zeros(1, self.img_size, self.img_size)\n",
    "        \n",
    "        return {\n",
    "            'image': image,      # RGB for teacher\n",
    "            'audio': audio,      # Audio for student\n",
    "            'depth_gt': depth,   # Ground truth (optional)\n",
    "            'filename': row['camera file name']\n",
    "        }\n",
    "    \n",
    "    def _load_audio(self, audio_path):\n",
    "        \"\"\"Load audio with multiple fallback methods (from BatvisionV2_Dataset.py)\"\"\"\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "        except (RuntimeError, ValueError) as e:\n",
    "            try:\n",
    "                waveform, sr = torchaudio.load(audio_path, backend=\"soundfile\")\n",
    "            except:\n",
    "                try:\n",
    "                    from scipy.io import wavfile\n",
    "                    sr, audio_data = wavfile.read(audio_path)\n",
    "                    if audio_data.ndim == 1:\n",
    "                        waveform = torch.from_numpy(audio_data).float().unsqueeze(0)\n",
    "                    else:\n",
    "                        waveform = torch.from_numpy(audio_data.T).float()\n",
    "                    if waveform.dtype == torch.int16:\n",
    "                        waveform = waveform / 32768.0\n",
    "                    elif waveform.dtype == torch.int32:\n",
    "                        waveform = waveform / 2147483648.0\n",
    "                except Exception as e2:\n",
    "                    try:\n",
    "                        import soundfile as sf\n",
    "                        audio_data, sr = sf.read(audio_path)\n",
    "                        if audio_data.ndim == 1:\n",
    "                            waveform = torch.from_numpy(audio_data).float().unsqueeze(0)\n",
    "                        else:\n",
    "                            waveform = torch.from_numpy(audio_data.T).float()\n",
    "                    except Exception as e3:\n",
    "                        raise RuntimeError(\n",
    "                            f\"Could not load audio file {audio_path} with any method. \"\n",
    "                            f\"Tried: torchaudio (error: {e}), scipy (error: {e2}), soundfile (error: {e3})\"\n",
    "                        )\n",
    "        return waveform, sr\n",
    "\n",
    "# Test dataset\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Testing DepthAnyAudioDataset\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test with single location or multiple locations based on USE_ALL_LOCATIONS\n",
    "if USE_ALL_LOCATIONS:\n",
    "    test_locations = locations  # Use predefined locations list\n",
    "else:\n",
    "    test_locations = LOCATION  # Single location\n",
    "\n",
    "test_dataset = DepthAnyAudioDataset(\n",
    "    root_dir=ROOT_DIR,\n",
    "    locations=test_locations,\n",
    "    split='train',\n",
    "    max_depth=MAX_DEPTH,\n",
    "    img_size=IMG_SIZE\n",
    ")\n",
    "\n",
    "sample = test_dataset[0]\n",
    "print(f\"\\nSample 0:\")\n",
    "print(f\"  Image shape: {sample['image'].shape}, range: [{sample['image'].min():.3f}, {sample['image'].max():.3f}]\")\n",
    "print(f\"  Audio shape: {sample['audio'].shape}, range: [{sample['audio'].min():.3f}, {sample['audio'].max():.3f}]\")\n",
    "print(f\"  Depth shape: {sample['depth_gt'].shape}, range: [{sample['depth_gt'].min():.3f}, {sample['depth_gt'].max():.3f}]\")\n",
    "print(f\"  Filename: {sample['filename']}\")\n",
    "print(\"âœ… Dataset test passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Loading Teacher Model: vitl_feature\n",
      "================================================================================\n",
      "Loading ViT-L Teacher (RGB input) from timm...\n",
      "âœ… Loaded ViT-L Teacher model: vit_large_patch16_224\n",
      "   Input: RGB images (3 channels)\n",
      "   Patch size: 16x16\n",
      "   Embedding dim: 1024\n",
      "âœ… Teacher model (ViT-L) frozen\n",
      "Teacher features extracted at layers: ['layer_final']\n",
      "  layer_final: shape torch.Size([1, 1024, 14, 14])\n",
      "âœ… ViT-L teacher model (RGB input) loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "if USE_DISTILLATION:\n",
    "    # Only load teacher model if distillation is enabled\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Loading Teacher Model: {TEACHER_MODEL}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if TEACHER_MODEL == 'vitl_feature':\n",
    "        # Load ViT-L for feature-level KD\n",
    "        # Teacher: RGB ì´ë¯¸ì§€ ìž…ë ¥ì˜ ì‚¬ì „í•™ìŠµ ViT-L\n",
    "        try:\n",
    "            import timm\n",
    "            print(f\"Loading ViT-L Teacher (RGB input) from timm...\")\n",
    "            \n",
    "            # ViT-L feature extractor wrapper for RGB images\n",
    "            class ViTLFeatureExtractor(nn.Module):\n",
    "                \"\"\"ViT-L model that extracts features at specified transformer blocks from RGB images\"\"\"\n",
    "                def __init__(self, model_name=None, feature_layers=[6, 12, 18, 24], input_channels=3):\n",
    "                    super().__init__()\n",
    "                    # Try different ViT-L model names if not specified\n",
    "                    if model_name is None:\n",
    "                        # Try common ViT-L model names in order of preference\n",
    "                        possible_models = [\n",
    "                            'vit_large_patch16_224',  # Most common, ImageNet-1k pretrained\n",
    "                            'vit_large_patch16_224_in21k',  # ImageNet-21k pretrained\n",
    "                            'vit_large_patch14_224_in21k',  # ImageNet-21k pretrained, patch14\n",
    "                            'vit_large_patch14_224',  # Less common\n",
    "                        ]\n",
    "                        model_name = None\n",
    "                        for m in possible_models:\n",
    "                            try:\n",
    "                                # Check if model exists in timm registry\n",
    "                                if m in timm.list_models('*vit_large*'):\n",
    "                                    model_name = m\n",
    "                                    break\n",
    "                            except:\n",
    "                                # Fallback: try to create model\n",
    "                                try:\n",
    "                                    test_model = timm.create_model(m, pretrained=False, num_classes=0, check_cfg=False)\n",
    "                                    model_name = m\n",
    "                                    del test_model\n",
    "                                    break\n",
    "                                except:\n",
    "                                    continue\n",
    "                        \n",
    "                        if model_name is None:\n",
    "                            # Last resort: try to find any vit_large model\n",
    "                            try:\n",
    "                                available_models = timm.list_models('*vit_large*')\n",
    "                                if available_models:\n",
    "                                    model_name = available_models[0]\n",
    "                                    print(f\"âš ï¸  Using first available ViT-L model: {model_name}\")\n",
    "                                else:\n",
    "                                    raise ValueError(\"No ViT-L models found in timm. Please install timm or specify model_name.\")\n",
    "                            except:\n",
    "                                raise ValueError(\"Could not find a valid ViT-L model. Please install timm or specify model_name.\")\n",
    "                    \n",
    "                    # Load pretrained ViT-L\n",
    "                    try:\n",
    "                        self.vit = timm.create_model(\n",
    "                            model_name,\n",
    "                            pretrained=True,\n",
    "                            num_classes=0,  # Remove classification head\n",
    "                            img_size=224\n",
    "                        )\n",
    "                    except RuntimeError as e:\n",
    "                        # If pretrained weights don't exist, try without pretrained\n",
    "                        print(f\"âš ï¸  Warning: Pretrained weights not found for {model_name}. Using random initialization.\")\n",
    "                        self.vit = timm.create_model(\n",
    "                            model_name,\n",
    "                            pretrained=False,\n",
    "                            num_classes=0,\n",
    "                            img_size=224\n",
    "                        )\n",
    "                    \n",
    "                    self.feature_layers = feature_layers\n",
    "                    self.model_name = model_name  # Store model name for reference\n",
    "                    self.input_channels = input_channels\n",
    "                    # Determine patch size from model\n",
    "                    if 'patch14' in model_name:\n",
    "                        self.patch_size = 14\n",
    "                    elif 'patch16' in model_name:\n",
    "                        self.patch_size = 16\n",
    "                    else:\n",
    "                        self.patch_size = 16  # Default\n",
    "                    self.embed_dim = 1024  # ViT-L embedding dimension\n",
    "                    \n",
    "                def forward(self, x, return_features=True):\n",
    "                    \"\"\"\n",
    "                    Args:\n",
    "                        x: [B, 3, H, W] - RGB image input\n",
    "                        return_features: If True, return intermediate features\n",
    "                    \n",
    "                    Returns:\n",
    "                        features: Dict of features at specified layers\n",
    "                        or output: Final output if return_features=False\n",
    "                    \"\"\"\n",
    "                    # Ensure RGB input (3 channels)\n",
    "                    if x.shape[1] != 3:\n",
    "                        if x.shape[1] == 1:\n",
    "                            x = x.repeat(1, 3, 1, 1)\n",
    "                        elif x.shape[1] == 2:\n",
    "                            # Convert 2-channel to 3-channel\n",
    "                            x = torch.cat([x, (x[:, 0:1] + x[:, 1:2]) / 2], dim=1)\n",
    "                    \n",
    "                    # Resize to ViT input size (224x224)\n",
    "                    if x.shape[-1] != 224:\n",
    "                        x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "                    \n",
    "                    # Normalize to ImageNet stats\n",
    "                    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(x.device)\n",
    "                    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(x.device)\n",
    "                    x = (x - mean) / std\n",
    "                    \n",
    "                    if return_features:\n",
    "                        # Extract features at specified layers\n",
    "                        features = {}\n",
    "                        \n",
    "                        # Patch embedding\n",
    "                        x = self.vit.patch_embed(x)  # [B, N, embed_dim]\n",
    "                        \n",
    "                        # Add cls token if exists\n",
    "                        if hasattr(self.vit, 'cls_token') and self.vit.cls_token is not None:\n",
    "                            cls_tokens = self.vit.cls_token.expand(x.shape[0], -1, -1)  # [B, 1, embed_dim]\n",
    "                            x = torch.cat([cls_tokens, x], dim=1)  # [B, N+1, embed_dim]\n",
    "                        \n",
    "                        # Add positional embedding\n",
    "                        if hasattr(self.vit, 'pos_embed') and self.vit.pos_embed is not None:\n",
    "                            # pos_embed shape: [1, N+1, embed_dim] (includes cls token)\n",
    "                            if x.shape[1] == self.vit.pos_embed.shape[1]:\n",
    "                                x = x + self.vit.pos_embed\n",
    "                            else:\n",
    "                                # Handle dynamic pos_embed (slice to match)\n",
    "                                x = x + self.vit.pos_embed[:, :x.shape[1], :]\n",
    "                        \n",
    "                        # Apply pos_drop if exists\n",
    "                        if hasattr(self.vit, 'pos_drop'):\n",
    "                            x = self.vit.pos_drop(x)\n",
    "                        \n",
    "                        # Pre-norm if exists\n",
    "                        if hasattr(self.vit, 'norm_pre'):\n",
    "                            x = self.vit.norm_pre(x)\n",
    "                        \n",
    "                        # Extract features at specified transformer blocks\n",
    "                        # blocks can be nn.ModuleList or nn.Sequential\n",
    "                        if isinstance(self.vit.blocks, nn.ModuleList):\n",
    "                            for i, block in enumerate(self.vit.blocks):\n",
    "                                x = block(x)\n",
    "                                if (i + 1) in self.feature_layers:\n",
    "                                    # Remove cls token if exists before reshaping\n",
    "                                    if hasattr(self.vit, 'cls_token') and self.vit.cls_token is not None:\n",
    "                                        # x shape: [B, N+1, C], remove first token\n",
    "                                        x_patches = x[:, 1:, :]  # [B, N, C]\n",
    "                                    else:\n",
    "                                        x_patches = x  # [B, N, C]\n",
    "                                    \n",
    "                                    # Reshape to spatial format: [B, N, C] -> [B, C, H, W]\n",
    "                                    B, N, C = x_patches.shape\n",
    "                                    # Calculate spatial dimensions from patch embedding\n",
    "                                    # For patch_size=14 and img_size=224: N = (224/14)^2 = 16^2 = 256\n",
    "                                    H = W = int(N ** 0.5)  # Assuming square patches\n",
    "                                    if H * W == N:\n",
    "                                        feat = x_patches.permute(0, 2, 1).view(B, C, H, W)\n",
    "                                        features[f'layer_{i+1}'] = feat\n",
    "                        else:\n",
    "                            # If blocks is Sequential, we need to hook into it\n",
    "                            # For now, just process all blocks\n",
    "                            x = self.vit.blocks(x)\n",
    "                            # Note: Sequential doesn't allow per-layer extraction easily\n",
    "                            # This is a fallback - ideally blocks should be ModuleList\n",
    "                            if hasattr(self.vit, 'cls_token') and self.vit.cls_token is not None:\n",
    "                                x_patches = x[:, 1:, :]\n",
    "                            else:\n",
    "                                x_patches = x\n",
    "                            B, N, C = x_patches.shape\n",
    "                            H = W = int(N ** 0.5)\n",
    "                            if H * W == N:\n",
    "                                feat = x_patches.permute(0, 2, 1).view(B, C, H, W)\n",
    "                                # Store as last layer if no specific layers found\n",
    "                                features['layer_final'] = feat\n",
    "                        \n",
    "                        return features\n",
    "                    else:\n",
    "                        # Use forward_features if available (more reliable)\n",
    "                        if hasattr(self.vit, 'forward_features'):\n",
    "                            x = self.vit.forward_features(x)\n",
    "                        else:\n",
    "                            # Manual forward pass\n",
    "                            x = self.vit.patch_embed(x)\n",
    "                            if hasattr(self.vit, 'pos_embed'):\n",
    "                                if x.shape[1] == self.vit.pos_embed.shape[1]:\n",
    "                                    x = x + self.vit.pos_embed\n",
    "                                else:\n",
    "                                    x = x + self.vit.pos_embed[:, :x.shape[1], :]\n",
    "                            if hasattr(self.vit, 'pos_drop'):\n",
    "                                x = self.vit.pos_drop(x)\n",
    "                            if hasattr(self.vit, 'norm_pre'):\n",
    "                                x = self.vit.norm_pre(x)\n",
    "                            x = self.vit.blocks(x)\n",
    "                            if hasattr(self.vit, 'norm'):\n",
    "                                x = self.vit.norm(x)\n",
    "                        return x\n",
    "            \n",
    "            # Create ViT-L teacher (RGB input)\n",
    "            teacher_model = ViTLFeatureExtractor(\n",
    "                model_name=None,  # Auto-detect available ViT-L model\n",
    "                feature_layers=FEATURE_KD_LAYERS,\n",
    "                input_channels=3  # RGB input\n",
    "            ).to(device)\n",
    "            \n",
    "            # Print which model was loaded\n",
    "            print(f\"âœ… Loaded ViT-L Teacher model: {teacher_model.model_name}\")\n",
    "            print(f\"   Input: RGB images (3 channels)\")\n",
    "            print(f\"   Patch size: {teacher_model.patch_size}x{teacher_model.patch_size}\")\n",
    "            print(f\"   Embedding dim: {teacher_model.embed_dim}\")\n",
    "            \n",
    "            # Freeze teacher\n",
    "            if FREEZE_TEACHER:\n",
    "                for param in teacher_model.parameters():\n",
    "                    param.requires_grad = False\n",
    "                teacher_model.eval()\n",
    "                print(\"âœ… Teacher model (ViT-L) frozen\")\n",
    "            \n",
    "            # Test teacher with RGB image\n",
    "            with torch.no_grad():\n",
    "                test_img = torch.randn(1, 3, 256, 256).to(device)\n",
    "                teacher_features = teacher_model(test_img, return_features=True)\n",
    "                print(f\"Teacher features extracted at layers: {list(teacher_features.keys())}\")\n",
    "                for k, v in teacher_features.items():\n",
    "                    print(f\"  {k}: shape {v.shape}\")\n",
    "            \n",
    "            print(\"âœ… ViT-L teacher model (RGB input) loaded successfully!\")\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"âš ï¸  timm not found. Installing...\")\n",
    "            print(\"Run: pip install timm\")\n",
    "            raise ImportError(\"timm is required for ViT-L feature extraction\")\n",
    "    \n",
    "    elif TEACHER_MODEL == 'depthanything_v2_vitl':\n",
    "        # Load Depth Anything V2 (original prediction-level KD)\n",
    "        try:\n",
    "            from depth_anything_v2.dpt import DepthAnythingV2\n",
    "        except ImportError:\n",
    "            print(\"âš ï¸  Depth Anything V2 not found. Installing...\")\n",
    "            print(\"Run: pip install depth-anything-v2\")\n",
    "            print(\"\\nFor now, we'll use a placeholder teacher model.\")\n",
    "            \n",
    "            # Placeholder teacher (ResNet-based)\n",
    "            class PlaceholderTeacher(nn.Module):\n",
    "                def __init__(self):\n",
    "                    super().__init__()\n",
    "                    resnet = models.resnet18(pretrained=True)\n",
    "                    self.encoder = nn.Sequential(*list(resnet.children())[:-2])\n",
    "                    self.decoder = nn.Sequential(\n",
    "                        nn.Conv2d(512, 256, 3, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "                        nn.Conv2d(256, 128, 3, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "                        nn.Conv2d(128, 64, 3, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "                        nn.Conv2d(64, 32, 3, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "                        nn.Conv2d(32, 1, 3, padding=1),\n",
    "                    )\n",
    "                \n",
    "                def forward(self, x):\n",
    "                    x = self.encoder(x)\n",
    "                    x = self.decoder(x)\n",
    "                    # Resize to match input\n",
    "                    x = F.interpolate(x, size=(256, 256), mode='bilinear', align_corners=False)\n",
    "                    return x * 30.0  # Scale to depth range\n",
    "            \n",
    "            DepthAnythingV2 = PlaceholderTeacher\n",
    "            USING_PLACEHOLDER_TEACHER = True\n",
    "        else:\n",
    "            USING_PLACEHOLDER_TEACHER = False\n",
    "\n",
    "        if USING_PLACEHOLDER_TEACHER:\n",
    "            print(\"âš ï¸  Using placeholder teacher (ResNet-based)\")\n",
    "            teacher_model = PlaceholderTeacher().to(device)\n",
    "        else:\n",
    "            print(f\"Loading Depth Anything V2: {TEACHER_ENCODER}\")\n",
    "            model_configs = {\n",
    "                'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "                'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "                'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n",
    "            }\n",
    "            \n",
    "            teacher_model = DepthAnythingV2(**model_configs[TEACHER_ENCODER])\n",
    "            teacher_model.load_state_dict(torch.load(f'checkpoints/depth_anything_v2_{TEACHER_ENCODER}.pth', map_location='cpu'))\n",
    "            teacher_model = teacher_model.to(device)\n",
    "\n",
    "        # Freeze teacher\n",
    "        if FREEZE_TEACHER:\n",
    "            for param in teacher_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            teacher_model.eval()\n",
    "            print(\"âœ… Teacher model frozen\")\n",
    "\n",
    "        # Test teacher\n",
    "        with torch.no_grad():\n",
    "            test_img = torch.randn(1, 3, 256, 256).to(device)\n",
    "            teacher_output = teacher_model(test_img)\n",
    "            print(f\"Teacher output shape: {teacher_output.shape}\")\n",
    "            print(f\"Teacher output range: [{teacher_output.min():.3f}, {teacher_output.max():.3f}]\")\n",
    "\n",
    "        print(\"âœ… Teacher model loaded successfully!\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown teacher model: {TEACHER_MODEL}\")\n",
    "else:\n",
    "    # Supervised learning mode: No teacher model needed\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Teacher Model: SKIPPED\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"ðŸ“š Supervised learning mode: Training with GT depth only\")\n",
    "    print(\"âš ï¸  Teacher model will not be loaded or used\")\n",
    "    teacher_model = None\n",
    "    print(\"âœ… Ready for supervised training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Model: Audio U-Net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Loading Student Model (Audio ViT-L with Spectrogram-Specialized Architecture)\n",
      "================================================================================\n",
      "âœ… Student ViT initialized from teacher weights\n",
      "âœ… Student model: Audio ViT-L with Spectrogram-Specialized Architecture\n",
      "   - Spectrogram-aware patch embedding (multi-layer conv)\n",
      "   - Spatial depth decoder (multi-scale feature fusion)\n",
      "   - Initialized from teacher ViT-L weights\n",
      "\n",
      "Student model: Audio ViT-L (Spectrogram-Specialized)\n",
      "Trainable parameters: 452,146,145\n",
      "\n",
      "Student test output:\n",
      "  Output shape: torch.Size([1, 1, 256, 256])\n",
      "  Output range: [-0.943, 2.067]\n",
      "  Features extracted at layers: ['layer_final']\n",
      "    layer_final: shape torch.Size([1, 1024, 13, 13])\n",
      "\n",
      "âœ… Student model (Audio ViT-L with Spectrogram-Specialized Architecture) created successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Loading Student Model (Audio ViT-L with Spectrogram-Specialized Architecture)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if USE_FEATURE_KD and TEACHER_MODEL == 'vitl_feature':\n",
    "    # Student: Audio ìŠ¤íŽ™íŠ¸ë¡œê·¸ëž¨ ìž…ë ¥ì˜ ViT-L (Teacherë¡œë¶€í„° ì´ˆê¸°í™”)\n",
    "    import timm\n",
    "    \n",
    "    # Audio ViT-L Student model with spectrogram-specialized architecture\n",
    "    class AudioViTLStudent(nn.Module):\n",
    "        \"\"\"\n",
    "        ViT-L Student model for audio spectrogram input, initialized from teacher.\n",
    "        Spectrogram-specialized architecture:\n",
    "        1. Spectrogram-aware patch embedding (2-channel input with multi-layer conv)\n",
    "        2. Spatial depth decoder (patch tokens -> depth map via multi-scale fusion)\n",
    "        3. Multi-scale feature fusion for better depth prediction\n",
    "        \"\"\"\n",
    "        def __init__(self, teacher_model, feature_layers=[6, 12, 18, 24], output_size=256):\n",
    "            super().__init__()\n",
    "            self.feature_layers = feature_layers\n",
    "            self.output_size = output_size\n",
    "            \n",
    "            # Create ViT-L backbone (same architecture as teacher)\n",
    "            # Use teacher's model name and structure\n",
    "            teacher_vit = teacher_model.vit\n",
    "            \n",
    "            # Create new ViT-L model with same architecture\n",
    "            self.vit = timm.create_model(\n",
    "                teacher_model.model_name,\n",
    "                pretrained=False,  # Will initialize from teacher\n",
    "                num_classes=0,\n",
    "                img_size=224\n",
    "            )\n",
    "            \n",
    "            # Initialize from teacher weights\n",
    "            self._init_from_teacher(teacher_vit)\n",
    "            \n",
    "            # Modify patch embedding to accept 2-channel audio input\n",
    "            # Original patch_embed expects 3 channels, we need to adapt it\n",
    "            original_patch_embed = self.vit.patch_embed\n",
    "            # timm's patch_embed is typically a Conv2d layer\n",
    "            if hasattr(original_patch_embed, 'proj'):\n",
    "                # PatchEmbed with proj attribute\n",
    "                original_proj = original_patch_embed.proj\n",
    "                embed_dim = original_proj.out_channels\n",
    "                kernel_size = original_proj.kernel_size[0] if isinstance(original_proj.kernel_size, tuple) else original_proj.kernel_size\n",
    "                stride = original_proj.stride[0] if isinstance(original_proj.stride, tuple) else original_proj.stride\n",
    "                padding = original_proj.padding[0] if isinstance(original_proj.padding, tuple) else original_proj.padding\n",
    "            else:\n",
    "                # Direct Conv2d\n",
    "                original_proj = original_patch_embed\n",
    "                embed_dim = original_proj.out_channels\n",
    "                kernel_size = original_proj.kernel_size[0] if isinstance(original_proj.kernel_size, tuple) else original_proj.kernel_size\n",
    "                stride = original_proj.stride[0] if isinstance(original_proj.stride, tuple) else original_proj.stride\n",
    "                padding = original_proj.padding[0] if isinstance(original_proj.padding, tuple) else original_proj.padding\n",
    "            \n",
    "            # ========== Spectrogram-Specialized Patch Embedding ==========\n",
    "            # Use multi-layer conv to better capture frequency-time patterns in spectrogram\n",
    "            self.audio_patch_embed = nn.Sequential(\n",
    "                nn.Conv2d(2, embed_dim // 2, kernel_size=3, stride=1, padding=1),  # Initial projection\n",
    "                nn.BatchNorm2d(embed_dim // 2),\n",
    "                nn.GELU(),\n",
    "                nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=kernel_size, stride=stride, padding=padding)  # Patch embedding\n",
    "            )\n",
    "            \n",
    "            # Initialize from RGB patch embedding (smart initialization)\n",
    "            with torch.no_grad():\n",
    "                if hasattr(original_patch_embed, 'proj'):\n",
    "                    rgb_weight = original_patch_embed.proj.weight.data  # [out_dim, 3, kernel, kernel]\n",
    "                    rgb_bias = original_patch_embed.proj.bias.data if original_patch_embed.proj.bias is not None else None\n",
    "                else:\n",
    "                    rgb_weight = original_patch_embed.weight.data\n",
    "                    rgb_bias = original_patch_embed.bias.data if original_patch_embed.bias is not None else None\n",
    "                \n",
    "                # First conv: 2 channels -> embed_dim//2 (use first 2 RGB channels)\n",
    "                audio_weight_1 = rgb_weight[:, :2, :, :].mean(dim=0, keepdim=True).repeat(embed_dim // 2, 1, 1, 1)\n",
    "                self.audio_patch_embed[0].weight.data = audio_weight_1\n",
    "                \n",
    "                # Second conv: embed_dim//2 -> embed_dim (initialize from RGB mean)\n",
    "                rgb_mean = rgb_weight.mean(dim=1, keepdim=True)  # [out_dim, 1, kernel, kernel]\n",
    "                audio_weight_2 = rgb_mean.repeat(1, embed_dim // 2, 1, 1)\n",
    "                self.audio_patch_embed[3].weight.data = audio_weight_2\n",
    "                if rgb_bias is not None and self.audio_patch_embed[3].bias is not None:\n",
    "                    self.audio_patch_embed[3].bias.data = rgb_bias.clone()\n",
    "            \n",
    "            # ========== Spatial Depth Decoder (Spectrogram-Specialized) ==========\n",
    "            # Instead of global pooling, decode patch tokens spatially for better depth prediction\n",
    "            self.embed_dim = 1024  # ViT-L embedding dimension\n",
    "            patch_size = teacher_model.patch_size  # 16 for vit_large_patch16_224\n",
    "            vit_h = vit_w = 224 // patch_size  # 14 for patch16_224\n",
    "            \n",
    "            # Multi-scale feature fusion decoder\n",
    "            # Fuse features from different transformer layers for better depth prediction\n",
    "            self.depth_decoder = nn.ModuleDict({\n",
    "                'projection': nn.ModuleList([\n",
    "                    nn.Conv2d(self.embed_dim, self.embed_dim // 2, 1) for _ in range(len(feature_layers))\n",
    "                ]),\n",
    "                'fusion': nn.Sequential(\n",
    "                    nn.Conv2d(self.embed_dim // 2 * len(feature_layers), self.embed_dim // 2, 3, padding=1),\n",
    "                    nn.BatchNorm2d(self.embed_dim // 2),\n",
    "                    nn.GELU(),\n",
    "                ),\n",
    "                'upsample': nn.Sequential(\n",
    "                    # vit_h x vit_w (14x14) -> output_size x output_size\n",
    "                    nn.ConvTranspose2d(self.embed_dim // 2, self.embed_dim // 4, kernel_size=4, stride=2, padding=1),  # 14->28\n",
    "                    nn.BatchNorm2d(self.embed_dim // 4),\n",
    "                    nn.GELU(),\n",
    "                    nn.ConvTranspose2d(self.embed_dim // 4, self.embed_dim // 8, kernel_size=4, stride=2, padding=1),  # 28->56\n",
    "                    nn.BatchNorm2d(self.embed_dim // 8),\n",
    "                    nn.GELU(),\n",
    "                    nn.ConvTranspose2d(self.embed_dim // 8, self.embed_dim // 16, kernel_size=4, stride=2, padding=1),  # 56->112\n",
    "                    nn.BatchNorm2d(self.embed_dim // 16),\n",
    "                    nn.GELU(),\n",
    "                    nn.ConvTranspose2d(self.embed_dim // 16, self.embed_dim // 32, kernel_size=4, stride=2, padding=1),  # 112->224\n",
    "                    nn.BatchNorm2d(self.embed_dim // 32),\n",
    "                    nn.GELU(),\n",
    "                    nn.Conv2d(self.embed_dim // 32, 64, 3, padding=1),\n",
    "                    nn.BatchNorm2d(64),\n",
    "                    nn.GELU(),\n",
    "                    nn.Conv2d(64, 32, 3, padding=1),\n",
    "                    nn.BatchNorm2d(32),\n",
    "                    nn.GELU(),\n",
    "                    nn.Conv2d(32, 1, 3, padding=1),  # Depth output\n",
    "                )\n",
    "            })\n",
    "            \n",
    "            # Final resize to exact output size if needed\n",
    "            if output_size != 224:\n",
    "                self.final_resize = True\n",
    "            else:\n",
    "                self.final_resize = False\n",
    "            \n",
    "        def _init_from_teacher(self, teacher_vit):\n",
    "            \"\"\"Initialize student ViT from teacher ViT weights\"\"\"\n",
    "            student_state = self.vit.state_dict()\n",
    "            teacher_state = teacher_vit.state_dict()\n",
    "            \n",
    "            # Copy matching weights (excluding patch_embed.proj which we'll adapt)\n",
    "            for key in student_state.keys():\n",
    "                if key in teacher_state:\n",
    "                    if 'patch_embed.proj' not in key:  # Skip patch embedding (different input channels)\n",
    "                        student_state[key] = teacher_state[key].clone()\n",
    "            \n",
    "            self.vit.load_state_dict(student_state)\n",
    "            print(\"âœ… Student ViT initialized from teacher weights\")\n",
    "        \n",
    "        def forward(self, x, return_features=False):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                x: [B, 2, H, W] - Audio spectrogram (2 channels: left, right)\n",
    "                return_features: If True, return intermediate features\n",
    "            \n",
    "            Returns:\n",
    "                depth: [B, 1, H, W] - Predicted depth map\n",
    "                features: Dict of intermediate features (if return_features=True)\n",
    "            \"\"\"\n",
    "            B = x.shape[0]\n",
    "            original_size = x.shape[-1]\n",
    "            \n",
    "            # Resize to ViT input size (224x224) for processing\n",
    "            if x.shape[-1] != 224:\n",
    "                x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "            \n",
    "            # Normalize spectrogram (already in [0, 1] from dataset)\n",
    "            # Use spectrogram-appropriate normalization\n",
    "            mean = torch.tensor([0.5, 0.5]).view(1, 2, 1, 1).to(x.device)\n",
    "            std = torch.tensor([0.5, 0.5]).view(1, 2, 1, 1).to(x.device)\n",
    "            x = (x - mean) / std\n",
    "            \n",
    "            # ========== Spectrogram Patch Embedding ==========\n",
    "            x_patches = self.audio_patch_embed(x)  # [B, embed_dim, H_patch, W_patch]\n",
    "            B, C, H_patch, W_patch = x_patches.shape\n",
    "            \n",
    "            # Flatten spatial dimensions: [B, embed_dim, H_patch, W_patch] -> [B, H_patch*W_patch, embed_dim]\n",
    "            x = x_patches.flatten(2).transpose(1, 2)  # [B, N, embed_dim]\n",
    "            \n",
    "            # ========== ViT Transformer Blocks ==========\n",
    "            # Add cls token if exists\n",
    "            if hasattr(self.vit, 'cls_token') and self.vit.cls_token is not None:\n",
    "                cls_tokens = self.vit.cls_token.expand(B, -1, -1)\n",
    "                x = torch.cat([cls_tokens, x], dim=1)\n",
    "            \n",
    "            # Add positional embedding\n",
    "            if hasattr(self.vit, 'pos_embed') and self.vit.pos_embed is not None:\n",
    "                if x.shape[1] == self.vit.pos_embed.shape[1]:\n",
    "                    x = x + self.vit.pos_embed\n",
    "                else:\n",
    "                    x = x + self.vit.pos_embed[:, :x.shape[1], :]\n",
    "            \n",
    "            # Apply pos_drop if exists\n",
    "            if hasattr(self.vit, 'pos_drop'):\n",
    "                x = self.vit.pos_drop(x)\n",
    "            \n",
    "            # Pre-norm if exists\n",
    "            if hasattr(self.vit, 'norm_pre'):\n",
    "                x = self.vit.norm_pre(x)\n",
    "            \n",
    "            # Extract features at specified transformer blocks\n",
    "            multi_scale_features = []\n",
    "            features = {} if return_features else None\n",
    "            \n",
    "            if isinstance(self.vit.blocks, nn.ModuleList):\n",
    "                for i, block in enumerate(self.vit.blocks):\n",
    "                    x = block(x)\n",
    "                    if (i + 1) in self.feature_layers:\n",
    "                        # Remove cls token if exists before reshaping\n",
    "                        if hasattr(self.vit, 'cls_token') and self.vit.cls_token is not None:\n",
    "                            x_patches = x[:, 1:, :]  # [B, N, C] (remove cls token)\n",
    "                        else:\n",
    "                            x_patches = x\n",
    "                        \n",
    "                        # Reshape to spatial format: [B, N, C] -> [B, C, H_patch, W_patch]\n",
    "                        B, N, C = x_patches.shape\n",
    "                        # Calculate spatial dimensions from patch grid\n",
    "                        # For ViT-L patch16_224: N should be (224/16)^2 = 14^2 = 196\n",
    "                        H_feat = W_feat = int(N ** 0.5)\n",
    "                        \n",
    "                        # Handle cases where N is not a perfect square (shouldn't happen, but just in case)\n",
    "                        if H_feat * W_feat == N:\n",
    "                            feat = x_patches.permute(0, 2, 1).view(B, C, H_feat, W_feat)\n",
    "                            \n",
    "                            # Store for feature KD\n",
    "                            if return_features:\n",
    "                                features[f'layer_{i+1}'] = feat\n",
    "                            \n",
    "                            # Project and store for depth decoder\n",
    "                            layer_idx = self.feature_layers.index(i + 1)\n",
    "                            feat_proj = self.depth_decoder['projection'][layer_idx](feat)\n",
    "                            multi_scale_features.append(feat_proj)\n",
    "                        else:\n",
    "                            # If N is not perfect square, use interpolation\n",
    "                            # This shouldn't happen for standard ViT, but handle it gracefully\n",
    "                            feat_flat = x_patches.permute(0, 2, 1)  # [B, C, N]\n",
    "                            # Reshape to approximate square\n",
    "                            H_feat = W_feat = int(torch.sqrt(torch.tensor(N, dtype=torch.float32)).item())\n",
    "                            # Pad or crop to make it square\n",
    "                            target_size = H_feat * W_feat\n",
    "                            if N > target_size:\n",
    "                                feat_flat = feat_flat[:, :, :target_size]\n",
    "                            else:\n",
    "                                padding = torch.zeros(B, C, target_size - N, device=feat_flat.device, dtype=feat_flat.dtype)\n",
    "                                feat_flat = torch.cat([feat_flat, padding], dim=2)\n",
    "                            feat = feat_flat.view(B, C, H_feat, W_feat)\n",
    "                            \n",
    "                            if return_features:\n",
    "                                features[f'layer_{i+1}'] = feat\n",
    "                            \n",
    "                            layer_idx = self.feature_layers.index(i + 1)\n",
    "                            feat_proj = self.depth_decoder['projection'][layer_idx](feat)\n",
    "                            multi_scale_features.append(feat_proj)\n",
    "            else:\n",
    "                # Fallback: process all blocks (if blocks is Sequential)\n",
    "                x = self.vit.blocks(x)\n",
    "                if hasattr(self.vit, 'cls_token') and self.vit.cls_token is not None:\n",
    "                    x_patches = x[:, 1:, :]\n",
    "                else:\n",
    "                    x_patches = x\n",
    "                B, N, C = x_patches.shape\n",
    "                H_feat = W_feat = int(N ** 0.5)\n",
    "                if H_feat * W_feat == N:\n",
    "                    feat = x_patches.permute(0, 2, 1).view(B, C, H_feat, W_feat)\n",
    "                    if return_features:\n",
    "                        features['layer_final'] = feat\n",
    "                    # Use final layer for depth prediction (use first projection)\n",
    "                    feat_proj = self.depth_decoder['projection'][0](feat)\n",
    "                    multi_scale_features.append(feat_proj)\n",
    "                else:\n",
    "                    # Handle non-square case\n",
    "                    feat_flat = x_patches.permute(0, 2, 1)\n",
    "                    H_feat = W_feat = int(torch.sqrt(torch.tensor(N, dtype=torch.float32)).item())\n",
    "                    target_size = H_feat * W_feat\n",
    "                    if N > target_size:\n",
    "                        feat_flat = feat_flat[:, :, :target_size]\n",
    "                    else:\n",
    "                        padding = torch.zeros(B, C, target_size - N, device=feat_flat.device, dtype=feat_flat.dtype)\n",
    "                        feat_flat = torch.cat([feat_flat, padding], dim=2)\n",
    "                    feat = feat_flat.view(B, C, H_feat, W_feat)\n",
    "                    if return_features:\n",
    "                        features['layer_final'] = feat\n",
    "                    feat_proj = self.depth_decoder['projection'][0](feat)\n",
    "                    multi_scale_features.append(feat_proj)\n",
    "            \n",
    "            # ========== Spatial Depth Decoder ==========\n",
    "            if len(multi_scale_features) > 0:\n",
    "                # Check if we have the expected number of features\n",
    "                expected_channels = (self.embed_dim // 2) * len(self.feature_layers)\n",
    "                if len(multi_scale_features) < len(self.feature_layers):\n",
    "                    # If some features are missing, pad with the last available feature\n",
    "                    while len(multi_scale_features) < len(self.feature_layers):\n",
    "                        multi_scale_features.append(multi_scale_features[-1])\n",
    "                \n",
    "                # Fuse multi-scale features\n",
    "                fused = torch.cat(multi_scale_features, dim=1)  # [B, C*num_layers, H, W]\n",
    "                \n",
    "                # Verify channel count matches\n",
    "                if fused.shape[1] != expected_channels:\n",
    "                    # If channel count doesn't match, use adaptive approach\n",
    "                    # Project to expected channels if needed\n",
    "                    if fused.shape[1] < expected_channels:\n",
    "                        # Pad with zeros or repeat last feature\n",
    "                        pad_channels = expected_channels - fused.shape[1]\n",
    "                        padding = torch.zeros(B, pad_channels, fused.shape[2], fused.shape[3], \n",
    "                                             device=fused.device, dtype=fused.dtype)\n",
    "                        fused = torch.cat([fused, padding], dim=1)\n",
    "                    else:\n",
    "                        # Take first expected_channels\n",
    "                        fused = fused[:, :expected_channels, :, :]\n",
    "                \n",
    "                fused = self.depth_decoder['fusion'](fused)  # [B, C//2, H, W]\n",
    "                \n",
    "                # Upsample to output size\n",
    "                depth = self.depth_decoder['upsample'](fused)  # [B, 1, 224, 224]\n",
    "                \n",
    "                # Resize to exact output size if needed\n",
    "                if self.final_resize and depth.shape[-1] != self.output_size:\n",
    "                    depth = F.interpolate(depth, size=(self.output_size, self.output_size), \n",
    "                                        mode='bilinear', align_corners=False)\n",
    "            else:\n",
    "                # Fallback: use final features from transformer\n",
    "                if hasattr(self.vit, 'norm'):\n",
    "                    x_final = self.vit.norm(x)\n",
    "                else:\n",
    "                    x_final = x\n",
    "                \n",
    "                # Remove cls token\n",
    "                if hasattr(self.vit, 'cls_token') and self.vit.cls_token is not None:\n",
    "                    x_patches = x_final[:, 1:, :]\n",
    "                else:\n",
    "                    x_patches = x_final\n",
    "                \n",
    "                B, N, C = x_patches.shape\n",
    "                H_feat = W_feat = int(N ** 0.5)\n",
    "                if H_feat * W_feat == N:\n",
    "                    feat = x_patches.permute(0, 2, 1).view(B, C, H_feat, W_feat)\n",
    "                    # Project to embed_dim // 2 for consistency\n",
    "                    feat_proj = nn.Conv2d(C, self.embed_dim // 2, 1).to(feat.device)(feat)\n",
    "                    # Use simple upsampling decoder\n",
    "                    depth = F.interpolate(feat_proj, size=(self.output_size, self.output_size), \n",
    "                                        mode='bilinear', align_corners=False)\n",
    "                    depth = nn.Conv2d(self.embed_dim // 2, 1, 3, padding=1).to(depth.device)(depth)\n",
    "                else:\n",
    "                    # Ultimate fallback: constant depth\n",
    "                    depth = torch.zeros(B, 1, self.output_size, self.output_size).to(x.device)\n",
    "            \n",
    "            if return_features:\n",
    "                return depth, features\n",
    "            else:\n",
    "                return depth\n",
    "    \n",
    "    # Create student model (initialized from teacher)\n",
    "    if teacher_model is not None:\n",
    "        student_model = AudioViTLStudent(\n",
    "            teacher_model=teacher_model,\n",
    "            feature_layers=FEATURE_KD_LAYERS,\n",
    "            output_size=IMG_SIZE\n",
    "        ).to(device)\n",
    "        print(\"âœ… Student model: Audio ViT-L with Spectrogram-Specialized Architecture\")\n",
    "        print(\"   - Spectrogram-aware patch embedding (multi-layer conv)\")\n",
    "        print(\"   - Spatial depth decoder (multi-scale feature fusion)\")\n",
    "        print(\"   - Initialized from teacher ViT-L weights\")\n",
    "    else:\n",
    "        raise ValueError(\"Teacher model must be loaded before creating student model\")\n",
    "    \n",
    "    # Count parameters\n",
    "    student_params = sum(p.numel() for p in student_model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nStudent model: Audio ViT-L (Spectrogram-Specialized)\")\n",
    "    print(f\"Trainable parameters: {student_params:,}\")\n",
    "    \n",
    "    # Test student\n",
    "    test_audio = torch.randn(1, 2, 256, 256).to(device)\n",
    "    student_output, student_features = student_model(test_audio, return_features=True)\n",
    "    print(f\"\\nStudent test output:\")\n",
    "    print(f\"  Output shape: {student_output.shape}\")\n",
    "    print(f\"  Output range: [{student_output.min():.3f}, {student_output.max():.3f}]\")\n",
    "    print(f\"  Features extracted at layers: {list(student_features.keys())}\")\n",
    "    for k, v in student_features.items():\n",
    "        print(f\"    {k}: shape {v.shape}\")\n",
    "    print(\"\\nâœ… Student model (Audio ViT-L with Spectrogram-Specialized Architecture) created successfully!\")\n",
    "else:\n",
    "    # Fallback to UNet if not using feature KD\n",
    "    from models.unetbaseline_model import define_G\n",
    "    from config_loader import load_config\n",
    "    \n",
    "    cfg = load_config(dataset_name='batvisionv2', mode='train')\n",
    "    student_model = define_G(\n",
    "        cfg=cfg,\n",
    "        input_nc=2,\n",
    "        output_nc=1,\n",
    "        ngf=STUDENT_BASE_CHANNELS,\n",
    "        netG=STUDENT_MODEL,\n",
    "        norm='batch',\n",
    "        use_dropout=False,\n",
    "        init_type='normal',\n",
    "        init_gain=0.02,\n",
    "        gpu_ids=[]\n",
    "    ).to(device)\n",
    "    \n",
    "    student_params = sum(p.numel() for p in student_model.parameters() if p.requires_grad)\n",
    "    print(f\"Student model: {STUDENT_MODEL}\")\n",
    "    print(f\"Trainable parameters: {student_params:,}\")\n",
    "    \n",
    "    test_audio = torch.randn(1, 2, 256, 256).to(device)\n",
    "    student_output = student_model(test_audio)\n",
    "    print(f\"Student output shape: {student_output.shape}\")\n",
    "    print(\"âœ… Student model created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distillation Loss Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Distillation Loss Function\n",
      "================================================================================\n",
      "Loss type: combined\n",
      "  Î»_L1: 0.5\n",
      "  Î»_SIlog: 0.5\n",
      "  SIlog_Î»: 0.85\n",
      "\n",
      "Loss Strategy:\n",
      "  ðŸŽ¯ Feature-level KD: COSINE loss (Î»=2.0)\n",
      "    Layers: [6, 12, 18, 24]\n",
      "  ðŸŽ¯ GT supervision: COMBINED loss (Î»=1.0)\n",
      "  Î»_GT (ground truth): 1.0\n",
      "âœ… Loss function created!\n"
     ]
    }
   ],
   "source": [
    "from utils_loss import SIlogLoss\n",
    "\n",
    "class FeatureKDLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Feature-level Knowledge Distillation Loss\n",
    "    Matches student features with teacher features at multiple layers\n",
    "    \"\"\"\n",
    "    def __init__(self, loss_type='mse', lambda_feature=1.0):\n",
    "        super().__init__()\n",
    "        self.loss_type = loss_type\n",
    "        self.lambda_feature = lambda_feature\n",
    "        \n",
    "        if loss_type == 'mse':\n",
    "            self.loss_fn = nn.MSELoss()\n",
    "        elif loss_type == 'cosine':\n",
    "            self.cosine_sim = nn.CosineSimilarity(dim=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown feature loss type: {loss_type}\")\n",
    "    \n",
    "    def compute_feature_loss(self, student_feat, teacher_feat):\n",
    "        \"\"\"\n",
    "        Compute feature matching loss between student and teacher features\n",
    "        \n",
    "        Args:\n",
    "            student_feat: [B, C_s, H_s, W_s] - Student feature map\n",
    "            teacher_feat: [B, C_t, H_t, W_t] - Teacher feature map\n",
    "        \n",
    "        Returns:\n",
    "            loss: Scalar loss value\n",
    "        \"\"\"\n",
    "        # Get batch size and spatial dimensions (define B early to avoid UnboundLocalError)\n",
    "        B = student_feat.shape[0]\n",
    "        \n",
    "        # Align spatial dimensions (interpolate teacher to student size)\n",
    "        if student_feat.shape[-2:] != teacher_feat.shape[-2:]:\n",
    "            teacher_feat = F.interpolate(\n",
    "                teacher_feat, \n",
    "                size=student_feat.shape[-2:], \n",
    "                mode='bilinear', \n",
    "                align_corners=False\n",
    "            )\n",
    "        \n",
    "        # Align channel dimensions using proper projection\n",
    "        if student_feat.shape[1] != teacher_feat.shape[1]:\n",
    "            C_s, H, W = student_feat.shape[1], student_feat.shape[2], student_feat.shape[3]\n",
    "            C_t = teacher_feat.shape[1]\n",
    "            \n",
    "            # Use simple channel selection/averaging to match dimensions\n",
    "            # This preserves spatial information better than adaptive pooling\n",
    "            if C_s > C_t:\n",
    "                # Reduce student channels: take first C_t channels (or average groups)\n",
    "                if C_s % C_t == 0:\n",
    "                    # Group and average\n",
    "                    group_size = C_s // C_t\n",
    "                    student_feat = student_feat.reshape(B, C_t, group_size, H, W).mean(dim=2)\n",
    "                else:\n",
    "                    # Take first C_t channels\n",
    "                    student_feat = student_feat[:, :C_t, :, :]\n",
    "            else:\n",
    "                # Reduce teacher channels: same strategy\n",
    "                if C_t % C_s == 0:\n",
    "                    group_size = C_t // C_s\n",
    "                    teacher_feat = teacher_feat.reshape(B, C_s, group_size, H, W).mean(dim=2)\n",
    "                else:\n",
    "                    teacher_feat = teacher_feat[:, :C_s, :, :]\n",
    "        \n",
    "        # Normalize features for stable training (important for KD!)\n",
    "        # Use reshape instead of view to handle non-contiguous tensors\n",
    "        student_feat_flat = student_feat.reshape(B, -1)\n",
    "        teacher_feat_flat = teacher_feat.reshape(B, -1)\n",
    "        student_feat_norm = F.normalize(student_feat_flat, dim=1).reshape_as(student_feat)\n",
    "        teacher_feat_norm = F.normalize(teacher_feat_flat, dim=1).reshape_as(teacher_feat)\n",
    "        \n",
    "        # Compute loss\n",
    "        if self.loss_type == 'mse':\n",
    "            # Use normalized features for better gradient flow\n",
    "            loss = self.loss_fn(student_feat_norm, teacher_feat_norm)\n",
    "        elif self.loss_type == 'cosine':\n",
    "            # Flatten spatial dimensions\n",
    "            student_flat = student_feat_norm.reshape(B, -1)  # [B, C*H*W]\n",
    "            teacher_flat = teacher_feat_norm.reshape(B, -1)  # [B, C*H*W]\n",
    "            # Compute cosine similarity\n",
    "            cosine_sim = F.cosine_similarity(student_flat, teacher_flat, dim=1).mean()\n",
    "            loss = 1.0 - cosine_sim  # Convert similarity to distance\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def forward(self, student_features, teacher_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            student_features: Dict of student features {layer_name: feature_map}\n",
    "            teacher_features: Dict of teacher features {layer_name: feature_map}\n",
    "        \n",
    "        Returns:\n",
    "            total_loss: Total feature KD loss\n",
    "            loss_dict: Dict of per-layer losses\n",
    "        \"\"\"\n",
    "        # Initialize total_loss as None, will be set to first layer_loss tensor\n",
    "        total_loss = None\n",
    "        loss_dict = {}\n",
    "        \n",
    "        # Match features at corresponding layers\n",
    "        # For now, we'll match by layer index\n",
    "        student_keys = sorted(student_features.keys())\n",
    "        teacher_keys = sorted(teacher_features.keys())\n",
    "        \n",
    "        # Match layers (simple 1-to-1 mapping)\n",
    "        num_layers = min(len(student_keys), len(teacher_keys))\n",
    "        for i in range(num_layers):\n",
    "            student_key = student_keys[i] if i < len(student_keys) else student_keys[-1]\n",
    "            teacher_key = teacher_keys[i] if i < len(teacher_keys) else teacher_keys[-1]\n",
    "            \n",
    "            student_feat = student_features[student_key]\n",
    "            teacher_feat = teacher_features[teacher_key]\n",
    "            \n",
    "            layer_loss = self.compute_feature_loss(student_feat, teacher_feat)\n",
    "            \n",
    "            # Initialize total_loss with first layer_loss (to get device and dtype)\n",
    "            if total_loss is None:\n",
    "                total_loss = layer_loss\n",
    "            else:\n",
    "                total_loss = total_loss + layer_loss\n",
    "            \n",
    "            # Store layer loss value (convert to float for logging)\n",
    "            if isinstance(layer_loss, torch.Tensor):\n",
    "                loss_dict[f'feature_kd_{i}'] = layer_loss.item()\n",
    "            else:\n",
    "                loss_dict[f'feature_kd_{i}'] = float(layer_loss)\n",
    "        \n",
    "        if total_loss is None:\n",
    "            # No layers matched, return zero loss\n",
    "            # Get device from first student feature if available\n",
    "            if student_keys and len(student_keys) > 0:\n",
    "                feat_device = student_features[student_keys[0]].device\n",
    "            elif teacher_keys and len(teacher_keys) > 0:\n",
    "                feat_device = teacher_features[teacher_keys[0]].device\n",
    "            else:\n",
    "                feat_device = torch.device('cpu')\n",
    "            total_loss = torch.tensor(0.0, device=feat_device, dtype=torch.float32)\n",
    "            loss_dict['feature_kd_total'] = 0.0\n",
    "        else:\n",
    "            total_loss = total_loss / max(num_layers, 1)  # Average over layers\n",
    "            # Store total loss value\n",
    "            if isinstance(total_loss, torch.Tensor):\n",
    "                loss_dict['feature_kd_total'] = total_loss.item()\n",
    "            else:\n",
    "                loss_dict['feature_kd_total'] = float(total_loss)\n",
    "        \n",
    "        # Ensure lambda_feature is a tensor on the same device\n",
    "        if isinstance(total_loss, torch.Tensor):\n",
    "            lambda_feature = torch.tensor(self.lambda_feature, device=total_loss.device, dtype=total_loss.dtype)\n",
    "            return total_loss * lambda_feature, loss_dict\n",
    "        else:\n",
    "            return total_loss * self.lambda_feature, loss_dict\n",
    "\n",
    "\n",
    "class DistillationLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-Modal Distillation Loss with GT Supervision\n",
    "    - Distillation: Learn from teacher's pseudo-labels (SILog only - scale invariant)\n",
    "    - Feature KD: Learn from teacher's features (for ViT-L teacher)\n",
    "    - GT Supervision: Learn from ground truth depth (Combined L1 + SILog)\n",
    "    - Weighted combination of all\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, loss_type='combined', lambda_l1=0.5, lambda_silog=0.5, silog_lambda=0.85,\n",
    "                 use_gt_supervision=True, lambda_distill=0.5, lambda_gt=0.5,\n",
    "                 use_feature_kd=False, lambda_feature_kd=1.0, feature_kd_loss_type='mse'):\n",
    "        super().__init__()\n",
    "        self.loss_type = loss_type\n",
    "        self.lambda_l1 = lambda_l1\n",
    "        self.lambda_silog = lambda_silog\n",
    "        self.use_gt_supervision = use_gt_supervision\n",
    "        self.lambda_distill = lambda_distill\n",
    "        self.lambda_gt = lambda_gt\n",
    "        self.use_feature_kd = use_feature_kd\n",
    "        \n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.silog_loss = SIlogLoss(lambda_scale=silog_lambda)\n",
    "        \n",
    "        if use_feature_kd:\n",
    "            self.feature_kd_loss = FeatureKDLoss(\n",
    "                loss_type=feature_kd_loss_type,\n",
    "                lambda_feature=lambda_feature_kd\n",
    "            )\n",
    "    \n",
    "    def compute_loss(self, pred, target, valid_mask=None, use_silog_only=False):\n",
    "        \"\"\"\n",
    "        Compute loss between prediction and target\n",
    "        \n",
    "        Args:\n",
    "            pred: Prediction [B, 1, H, W]\n",
    "            target: Target [B, 1, H, W]\n",
    "            valid_mask: Valid pixel mask [B, 1, H, W]\n",
    "            use_silog_only: If True, only use SILog (for pseudo-labels with scale mismatch)\n",
    "        \"\"\"\n",
    "        if valid_mask is None:\n",
    "            valid_mask = (target > 0) & (target < 100)  # Reasonable depth range\n",
    "        \n",
    "        pred_valid = pred[valid_mask]\n",
    "        target_valid = target[valid_mask]\n",
    "        \n",
    "        if len(pred_valid) == 0:\n",
    "            return torch.tensor(0.0).to(pred.device), {}\n",
    "        \n",
    "        # For pseudo-labels (teacher): use SILog only (scale invariant)\n",
    "        if use_silog_only:\n",
    "            loss = self.silog_loss(pred_valid, target_valid)\n",
    "            loss_dict = {'silog': loss.item()}\n",
    "            return loss, loss_dict\n",
    "        \n",
    "        # For GT: use specified loss type\n",
    "        if self.loss_type == 'l1':\n",
    "            loss = self.l1_loss(pred_valid, target_valid)\n",
    "            loss_dict = {'l1': loss.item()}\n",
    "        elif self.loss_type == 'silog':\n",
    "            loss = self.silog_loss(pred_valid, target_valid)\n",
    "            loss_dict = {'silog': loss.item()}\n",
    "        elif self.loss_type == 'combined':\n",
    "            l1 = self.l1_loss(pred_valid, target_valid)\n",
    "            silog = self.silog_loss(pred_valid, target_valid)\n",
    "            loss = self.lambda_l1 * l1 + self.lambda_silog * silog\n",
    "            loss_dict = {'l1': l1.item(), 'silog': silog.item(), 'subtotal': loss.item()}\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown loss type: {self.loss_type}\")\n",
    "        \n",
    "        return loss, loss_dict\n",
    "    \n",
    "    def forward(self, pred, teacher_target=None, gt_target=None, \n",
    "                student_features=None, teacher_features=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred: Student prediction [B, 1, H, W]\n",
    "            teacher_target: Teacher pseudo-label [B, 1, H, W] (optional, for prediction-level KD)\n",
    "            gt_target: Ground truth depth [B, 1, H, W] (optional)\n",
    "            student_features: Dict of student features (optional, for feature-level KD)\n",
    "            teacher_features: Dict of teacher features (optional, for feature-level KD)\n",
    "        \"\"\"\n",
    "        # Initialize total_loss as None, will be set to first loss tensor\n",
    "        total_loss = None\n",
    "        loss_dict = {}\n",
    "        \n",
    "        # Feature-level KD loss (for ViT-L teacher)\n",
    "        if self.use_feature_kd and student_features is not None and teacher_features is not None:\n",
    "            feature_kd_loss, feature_kd_dict = self.feature_kd_loss(student_features, teacher_features)\n",
    "            # Initialize total_loss with first loss tensor\n",
    "            if total_loss is None:\n",
    "                total_loss = feature_kd_loss\n",
    "            else:\n",
    "                total_loss = total_loss + feature_kd_loss\n",
    "            loss_dict.update(feature_kd_dict)\n",
    "        \n",
    "        # Distillation loss (teacher pseudo-label) - SILog only (scale invariant)\n",
    "        if teacher_target is not None:\n",
    "            distill_loss, distill_dict = self.compute_loss(pred, teacher_target, use_silog_only=True)\n",
    "            # Convert lambda to tensor if needed\n",
    "            if isinstance(distill_loss, torch.Tensor):\n",
    "                lambda_distill = torch.tensor(self.lambda_distill, device=distill_loss.device, dtype=distill_loss.dtype)\n",
    "            else:\n",
    "                lambda_distill = self.lambda_distill\n",
    "            \n",
    "            if total_loss is None:\n",
    "                total_loss = lambda_distill * distill_loss\n",
    "            else:\n",
    "                total_loss = total_loss + lambda_distill * distill_loss\n",
    "            loss_dict['distill'] = distill_loss.item() if isinstance(distill_loss, torch.Tensor) else float(distill_loss)\n",
    "            for k, v in distill_dict.items():\n",
    "                loss_dict[f'distill_{k}'] = v\n",
    "        \n",
    "        # GT supervision loss - Combined loss (L1 + SILog)\n",
    "        if self.use_gt_supervision and gt_target is not None:\n",
    "            gt_loss, gt_dict = self.compute_loss(pred, gt_target, use_silog_only=False)\n",
    "            # Convert lambda to tensor if needed\n",
    "            if isinstance(gt_loss, torch.Tensor):\n",
    "                lambda_gt = torch.tensor(self.lambda_gt, device=gt_loss.device, dtype=gt_loss.dtype)\n",
    "            else:\n",
    "                lambda_gt = self.lambda_gt\n",
    "            \n",
    "            if total_loss is None:\n",
    "                total_loss = lambda_gt * gt_loss\n",
    "            else:\n",
    "                total_loss = total_loss + lambda_gt * gt_loss\n",
    "            loss_dict['gt'] = gt_loss.item() if isinstance(gt_loss, torch.Tensor) else float(gt_loss)\n",
    "            for k, v in gt_dict.items():\n",
    "                loss_dict[f'gt_{k}'] = v\n",
    "        \n",
    "        # If no losses were computed, return zero loss\n",
    "        if total_loss is None:\n",
    "            total_loss = torch.tensor(0.0, device=pred.device, dtype=pred.dtype)\n",
    "            loss_dict['total'] = 0.0\n",
    "        else:\n",
    "            loss_dict['total'] = total_loss.item()\n",
    "        \n",
    "        return total_loss, loss_dict\n",
    "\n",
    "# Create loss function\n",
    "distill_loss_fn = DistillationLoss(\n",
    "    loss_type=DISTILLATION_LOSS,\n",
    "    lambda_l1=LAMBDA_L1,\n",
    "    lambda_silog=LAMBDA_SILOG,\n",
    "    silog_lambda=SILOG_LAMBDA,\n",
    "    use_gt_supervision=USE_GT_SUPERVISION,\n",
    "    lambda_distill=LAMBDA_DISTILL,\n",
    "    lambda_gt=LAMBDA_GT,\n",
    "    use_feature_kd=USE_FEATURE_KD and TEACHER_MODEL == 'vitl_feature',\n",
    "    lambda_feature_kd=FEATURE_KD_LAMBDA,\n",
    "    feature_kd_loss_type=FEATURE_KD_LOSS_TYPE\n",
    ").to(device)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Distillation Loss Function\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Loss type: {DISTILLATION_LOSS}\")\n",
    "if DISTILLATION_LOSS == 'combined':\n",
    "    print(f\"  Î»_L1: {LAMBDA_L1}\")\n",
    "    print(f\"  Î»_SIlog: {LAMBDA_SILOG}\")\n",
    "    print(f\"  SIlog_Î»: {SILOG_LAMBDA}\")\n",
    "print(\"\")\n",
    "print(\"Loss Strategy:\")\n",
    "if USE_FEATURE_KD and TEACHER_MODEL == 'vitl_feature':\n",
    "    print(f\"  ðŸŽ¯ Feature-level KD: {FEATURE_KD_LOSS_TYPE.upper()} loss (Î»={FEATURE_KD_LAMBDA})\")\n",
    "    print(f\"    Layers: {FEATURE_KD_LAYERS}\")\n",
    "    print(f\"  ðŸŽ¯ GT supervision: {DISTILLATION_LOSS.upper()} loss (Î»={LAMBDA_GT})\")\n",
    "else:\n",
    "    print(f\"  ðŸŽ¯ Distillation (pseudo-label): SILog only (scale invariant, Î»={LAMBDA_DISTILL})\")\n",
    "    print(f\"  ðŸŽ¯ GT supervision: {DISTILLATION_LOSS.upper()} loss (Î»={LAMBDA_GT})\")\n",
    "if USE_GT_SUPERVISION:\n",
    "    print(f\"  Î»_GT (ground truth): {LAMBDA_GT}\")\n",
    "else:\n",
    "    print(f\"GT Supervision: Disabled\")\n",
    "print(\"âœ… Loss function created!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Feature Extraction & Loss Calculation Check ==========\n",
    "if USE_FEATURE_KD and TEACHER_MODEL == 'vitl_feature' and teacher_model is not None and student_model is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ” Feature Extraction & Loss Calculation Check\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create test data\n",
    "    test_img = torch.randn(1, 3, 256, 256).to(device)\n",
    "    test_audio = torch.randn(1, 2, 256, 256).to(device)\n",
    "    test_depth_gt = torch.randn(1, 1, 256, 256).to(device) * 10.0 + 5.0  # Reasonable depth range\n",
    "    \n",
    "    print(\"\\n1ï¸âƒ£ Teacher Feature Extraction Check:\")\n",
    "    print(f\"   Expected feature layers: {FEATURE_KD_LAYERS}\")\n",
    "    teacher_model.eval()\n",
    "    with torch.no_grad():\n",
    "        teacher_features = teacher_model(test_img, return_features=True)\n",
    "    print(f\"   âœ… Teacher features extracted: {list(teacher_features.keys())}\")\n",
    "    for k, v in teacher_features.items():\n",
    "        print(f\"      {k}: shape {v.shape}, range [{v.min():.3f}, {v.max():.3f}]\")\n",
    "    \n",
    "    print(\"\\n2ï¸âƒ£ Student Feature Extraction Check:\")\n",
    "    print(f\"   Expected feature layers: {FEATURE_KD_LAYERS}\")\n",
    "    student_model.eval()\n",
    "    with torch.no_grad():\n",
    "        student_output, student_features = student_model(test_audio, return_features=True)\n",
    "    print(f\"   âœ… Student features extracted: {list(student_features.keys())}\")\n",
    "    for k, v in student_features.items():\n",
    "        print(f\"      {k}: shape {v.shape}, range [{v.min():.3f}, {v.max():.3f}]\")\n",
    "    \n",
    "    print(\"\\n3ï¸âƒ£ Feature Matching Check:\")\n",
    "    student_keys = sorted(student_features.keys())\n",
    "    teacher_keys = sorted(teacher_features.keys())\n",
    "    print(f\"   Student keys: {student_keys}\")\n",
    "    print(f\"   Teacher keys: {teacher_keys}\")\n",
    "    \n",
    "    matched_layers = []\n",
    "    for layer_idx in FEATURE_KD_LAYERS:\n",
    "        student_key = f'layer_{layer_idx}'\n",
    "        teacher_key = f'layer_{layer_idx}'\n",
    "        if student_key in student_features and teacher_key in teacher_features:\n",
    "            matched_layers.append((student_key, teacher_key))\n",
    "            print(f\"   âœ… Layer {layer_idx}: Matched!\")\n",
    "            s_feat = student_features[student_key]\n",
    "            t_feat = teacher_features[teacher_key]\n",
    "            print(f\"      Student: {s_feat.shape}, Teacher: {t_feat.shape}\")\n",
    "        else:\n",
    "            print(f\"   âŒ Layer {layer_idx}: NOT MATCHED!\")\n",
    "            if student_key not in student_features:\n",
    "                print(f\"      Student key '{student_key}' not found!\")\n",
    "            if teacher_key not in teacher_features:\n",
    "                print(f\"      Teacher key '{teacher_key}' not found!\")\n",
    "    \n",
    "    print(f\"\\n   Total matched layers: {len(matched_layers)}/{len(FEATURE_KD_LAYERS)}\")\n",
    "    \n",
    "    print(\"\\n4ï¸âƒ£ Feature KD Loss Calculation Check:\")\n",
    "    if USE_FEATURE_KD:\n",
    "        feature_kd_loss_fn = FeatureKDLoss(\n",
    "            loss_type=FEATURE_KD_LOSS_TYPE,\n",
    "            lambda_feature=FEATURE_KD_LAMBDA\n",
    "        ).to(device)\n",
    "        \n",
    "        feature_kd_loss, feature_kd_dict = feature_kd_loss_fn(student_features, teacher_features)\n",
    "        print(f\"   âœ… Feature KD loss calculated: {feature_kd_loss.item():.6f}\")\n",
    "        print(f\"   Loss breakdown:\")\n",
    "        for k, v in feature_kd_dict.items():\n",
    "            print(f\"      {k}: {v:.6f}\")\n",
    "        \n",
    "        if feature_kd_loss.item() == 0.0:\n",
    "            print(\"   âš ï¸  WARNING: Feature KD loss is 0! Features may not be matched properly.\")\n",
    "        elif feature_kd_loss.item() > 100:\n",
    "            print(\"   âš ï¸  WARNING: Feature KD loss is very large! May need normalization adjustment.\")\n",
    "        else:\n",
    "            print(\"   âœ… Feature KD loss is in reasonable range.\")\n",
    "    \n",
    "    print(\"\\n5ï¸âƒ£ Full Distillation Loss Check:\")\n",
    "    # Forward pass\n",
    "    student_model.train()\n",
    "    student_output, student_features = student_model(test_audio, return_features=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        teacher_features = teacher_model(test_img, return_features=True)\n",
    "    \n",
    "    total_loss, loss_dict = distill_loss_fn(\n",
    "        pred=student_output,\n",
    "        teacher_target=None,\n",
    "        gt_target=test_depth_gt,\n",
    "        student_features=student_features,\n",
    "        teacher_features=teacher_features\n",
    "    )\n",
    "    \n",
    "    print(f\"   âœ… Total loss: {total_loss.item():.6f}\")\n",
    "    print(f\"   Loss breakdown:\")\n",
    "    for k, v in loss_dict.items():\n",
    "        if isinstance(v, (int, float)):\n",
    "            print(f\"      {k}: {v:.6f}\")\n",
    "        else:\n",
    "            print(f\"      {k}: {v}\")\n",
    "    \n",
    "    print(\"\\n6ï¸âƒ£ Loss Weight Summary:\")\n",
    "    print(f\"   Î»_feature_KD: {FEATURE_KD_LAMBDA}\")\n",
    "    print(f\"   Î»_GT: {LAMBDA_GT}\")\n",
    "    print(f\"   Î»_distill: {LAMBDA_DISTILL}\")\n",
    "    if 'feature_kd_total' in loss_dict:\n",
    "        feature_kd_contribution = loss_dict['feature_kd_total'] * FEATURE_KD_LAMBDA\n",
    "        gt_contribution = loss_dict.get('gt', 0) * LAMBDA_GT\n",
    "        print(f\"   Feature KD contribution: {feature_kd_contribution:.6f}\")\n",
    "        print(f\"   GT contribution: {gt_contribution:.6f}\")\n",
    "        if feature_kd_contribution > 0:\n",
    "            ratio = gt_contribution / feature_kd_contribution if feature_kd_contribution > 0 else float('inf')\n",
    "            print(f\"   GT/Feature_KD ratio: {ratio:.3f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ… Feature Extraction & Loss Calculation Check Complete!\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"âš ï¸  Feature KD check skipped (not using feature KD or models not loaded)\")\n",
    "\n",
    "## ðŸš€ Depth Any Audio Training Loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Feature Extraction & Loss Calculation Check ==========\n",
    "if USE_FEATURE_KD and TEACHER_MODEL == 'vitl_feature' and teacher_model is not None and student_model is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ” Feature Extraction & Loss Calculation Check\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create test data\n",
    "    test_img = torch.randn(1, 3, 256, 256).to(device)\n",
    "    test_audio = torch.randn(1, 2, 256, 256).to(device)\n",
    "    test_depth_gt = torch.randn(1, 1, 256, 256).to(device) * 10.0 + 5.0  # Reasonable depth range\n",
    "    \n",
    "    print(\"\\n1ï¸âƒ£ Teacher Feature Extraction Check:\")\n",
    "    print(f\"   Expected feature layers: {FEATURE_KD_LAYERS}\")\n",
    "    teacher_model.eval()\n",
    "    with torch.no_grad():\n",
    "        teacher_features = teacher_model(test_img, return_features=True)\n",
    "    print(f\"   âœ… Teacher features extracted: {list(teacher_features.keys())}\")\n",
    "    for k, v in teacher_features.items():\n",
    "        print(f\"      {k}: shape {v.shape}, range [{v.min():.3f}, {v.max():.3f}]\")\n",
    "    \n",
    "    print(\"\\n2ï¸âƒ£ Student Feature Extraction Check:\")\n",
    "    print(f\"   Expected feature layers: {FEATURE_KD_LAYERS}\")\n",
    "    student_model.eval()\n",
    "    with torch.no_grad():\n",
    "        student_output, student_features = student_model(test_audio, return_features=True)\n",
    "    print(f\"   âœ… Student features extracted: {list(student_features.keys())}\")\n",
    "    for k, v in student_features.items():\n",
    "        print(f\"      {k}: shape {v.shape}, range [{v.min():.3f}, {v.max():.3f}]\")\n",
    "    \n",
    "    print(\"\\n3ï¸âƒ£ Feature Matching Check:\")\n",
    "    student_keys = sorted(student_features.keys())\n",
    "    teacher_keys = sorted(teacher_features.keys())\n",
    "    print(f\"   Student keys: {student_keys}\")\n",
    "    print(f\"   Teacher keys: {teacher_keys}\")\n",
    "    \n",
    "    matched_layers = []\n",
    "    for layer_idx in FEATURE_KD_LAYERS:\n",
    "        student_key = f'layer_{layer_idx}'\n",
    "        teacher_key = f'layer_{layer_idx}'\n",
    "        if student_key in student_features and teacher_key in teacher_features:\n",
    "            matched_layers.append((student_key, teacher_key))\n",
    "            print(f\"   âœ… Layer {layer_idx}: Matched!\")\n",
    "            s_feat = student_features[student_key]\n",
    "            t_feat = teacher_features[teacher_key]\n",
    "            print(f\"      Student: {s_feat.shape}, Teacher: {t_feat.shape}\")\n",
    "        else:\n",
    "            print(f\"   âŒ Layer {layer_idx}: NOT MATCHED!\")\n",
    "            if student_key not in student_features:\n",
    "                print(f\"      Student key '{student_key}' not found!\")\n",
    "            if teacher_key not in teacher_features:\n",
    "                print(f\"      Teacher key '{teacher_key}' not found!\")\n",
    "    \n",
    "    print(f\"\\n   Total matched layers: {len(matched_layers)}/{len(FEATURE_KD_LAYERS)}\")\n",
    "    \n",
    "    print(\"\\n4ï¸âƒ£ Feature KD Loss Calculation Check:\")\n",
    "    if USE_FEATURE_KD:\n",
    "        feature_kd_loss_fn = FeatureKDLoss(\n",
    "            loss_type=FEATURE_KD_LOSS_TYPE,\n",
    "            lambda_feature=FEATURE_KD_LAMBDA\n",
    "        ).to(device)\n",
    "        \n",
    "        feature_kd_loss, feature_kd_dict = feature_kd_loss_fn(student_features, teacher_features)\n",
    "        print(f\"   âœ… Feature KD loss calculated: {feature_kd_loss.item():.6f}\")\n",
    "        print(f\"   Loss breakdown:\")\n",
    "        for k, v in feature_kd_dict.items():\n",
    "            print(f\"      {k}: {v:.6f}\")\n",
    "        \n",
    "        if feature_kd_loss.item() == 0.0:\n",
    "            print(\"   âš ï¸  WARNING: Feature KD loss is 0! Features may not be matched properly.\")\n",
    "        elif feature_kd_loss.item() > 100:\n",
    "            print(\"   âš ï¸  WARNING: Feature KD loss is very large! May need normalization adjustment.\")\n",
    "        else:\n",
    "            print(\"   âœ… Feature KD loss is in reasonable range.\")\n",
    "    \n",
    "    print(\"\\n5ï¸âƒ£ Full Distillation Loss Check:\")\n",
    "    # Forward pass\n",
    "    student_model.train()\n",
    "    student_output, student_features = student_model(test_audio, return_features=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        teacher_features = teacher_model(test_img, return_features=True)\n",
    "    \n",
    "    total_loss, loss_dict = distill_loss_fn(\n",
    "        pred=student_output,\n",
    "        teacher_target=None,\n",
    "        gt_target=test_depth_gt,\n",
    "        student_features=student_features,\n",
    "        teacher_features=teacher_features\n",
    "    )\n",
    "    \n",
    "    print(f\"   âœ… Total loss: {total_loss.item():.6f}\")\n",
    "    print(f\"   Loss breakdown:\")\n",
    "    for k, v in loss_dict.items():\n",
    "        if isinstance(v, (int, float)):\n",
    "            print(f\"      {k}: {v:.6f}\")\n",
    "        else:\n",
    "            print(f\"      {k}: {v}\")\n",
    "    \n",
    "    print(\"\\n6ï¸âƒ£ Loss Weight Summary:\")\n",
    "    print(f\"   Î»_feature_KD: {FEATURE_KD_LAMBDA}\")\n",
    "    print(f\"   Î»_GT: {LAMBDA_GT}\")\n",
    "    print(f\"   Î»_distill: {LAMBDA_DISTILL}\")\n",
    "    if 'feature_kd_total' in loss_dict:\n",
    "        feature_kd_contribution = loss_dict['feature_kd_total'] * FEATURE_KD_LAMBDA\n",
    "        gt_contribution = loss_dict.get('gt', 0) * LAMBDA_GT\n",
    "        print(f\"   Feature KD contribution: {feature_kd_contribution:.6f}\")\n",
    "        print(f\"   GT contribution: {gt_contribution:.6f}\")\n",
    "        if feature_kd_contribution > 0:\n",
    "            ratio = gt_contribution / feature_kd_contribution if feature_kd_contribution > 0 else float('inf')\n",
    "            print(f\"   GT/Feature_KD ratio: {ratio:.3f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ… Feature Extraction & Loss Calculation Check Complete!\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"âš ï¸  Feature KD check skipped (not using feature KD or models not loaded)\")\n",
    "\n",
    "## ðŸš€ Depth Any Audio Training Loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“¦ Creating Dataloaders\n",
      "================================================================================\n",
      "Loading ALL locations for distillation... (7 locations)\n",
      "Loaded 431 samples from 2ndFloorLuxembourg (train)\n",
      "Loaded 290 samples from 3rd_Floor_Luxembourg (train)\n",
      "Loaded 37 samples from Attic (train)\n",
      "Loaded 377 samples from Outdoor_Cobblestone_Path (train)\n",
      "Loaded 116 samples from Salle_Chevalier (train)\n",
      "Loaded 240 samples from Salle_des_Colonnes (train)\n",
      "Loaded 420 samples from V119_Cake_Corridors (train)\n",
      "âœ… Total: 1911 samples from 7 location(s)\n",
      "Loaded 132 samples from 2ndFloorLuxembourg (val)\n",
      "Loaded 98 samples from 3rd_Floor_Luxembourg (val)\n",
      "Loaded 12 samples from Attic (val)\n",
      "Loaded 125 samples from Outdoor_Cobblestone_Path (val)\n",
      "Loaded 40 samples from Salle_Chevalier (val)\n",
      "Loaded 80 samples from Salle_des_Colonnes (val)\n",
      "Loaded 138 samples from V119_Cake_Corridors (val)\n",
      "âœ… Total: 625 samples from 7 location(s)\n",
      "\n",
      "âœ… Dataloaders created:\n",
      "  Train: 1911 samples, 956 batches\n",
      "  Val: 625 samples, 313 batches\n",
      "\n",
      "âœ… Training setup complete!\n",
      "  Optimizer: AdamW (lr=0.0001, wd=0.0001)\n",
      "  Scheduler: CosineAnnealingLR\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“¦ Creating Dataloaders\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Determine locations based on USE_ALL_LOCATIONS\n",
    "if USE_ALL_LOCATIONS:\n",
    "    print(f\"Loading ALL locations for distillation... ({len(locations)} locations)\")\n",
    "    train_locations = locations  # Use predefined locations list\n",
    "    val_locations = locations\n",
    "else:\n",
    "    print(f\"Loading single location: {LOCATION}\")\n",
    "    train_locations = LOCATION\n",
    "    val_locations = LOCATION\n",
    "\n",
    "# Create datasets (now supports multiple locations internally)\n",
    "train_dataset = DepthAnyAudioDataset(\n",
    "    root_dir=ROOT_DIR,\n",
    "    locations=train_locations,\n",
    "    split='train',\n",
    "    max_depth=MAX_DEPTH,\n",
    "    img_size=IMG_SIZE\n",
    ")\n",
    "\n",
    "val_dataset = DepthAnyAudioDataset(\n",
    "    root_dir=ROOT_DIR,\n",
    "    locations=val_locations,\n",
    "    split='val',\n",
    "    max_depth=MAX_DEPTH,\n",
    "    img_size=IMG_SIZE\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=DISTILL_BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=DISTILL_BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=False)\n",
    "\n",
    "print(f\"\\nâœ… Dataloaders created:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples, {len(train_loader)} batches\")\n",
    "print(f\"  Val: {len(val_dataset)} samples, {len(val_loader)} batches\")\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(student_model.parameters(), lr=DISTILL_LR, weight_decay=DISTILL_WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=DISTILL_EPOCHS)\n",
    "\n",
    "print(f\"\\nâœ… Training setup complete!\")\n",
    "print(f\"  Optimizer: AdamW (lr={DISTILL_LR}, wd={DISTILL_WEIGHT_DECAY})\")\n",
    "print(f\"  Scheduler: CosineAnnealingLR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_depth_to_gt(teacher_depth, max_depth=30.0):\n",
    "    \"\"\"\n",
    "    Normalize teacher depth to match GT scale [0, max_depth]\n",
    "    Using min-max normalization per batch\n",
    "    \n",
    "    Args:\n",
    "        teacher_depth: [B, 1, H, W] teacher depth predictions\n",
    "        max_depth: Maximum depth value (default: 30.0m)\n",
    "    \n",
    "    Returns:\n",
    "        Normalized depth in range [0, max_depth]\n",
    "    \"\"\"\n",
    "    normalized = torch.zeros_like(teacher_depth)\n",
    "    for b in range(teacher_depth.shape[0]):\n",
    "        t_min = teacher_depth[b].min()\n",
    "        t_max = teacher_depth[b].max()\n",
    "        if t_max > t_min:\n",
    "            # Normalize to [0, max_depth]\n",
    "            normalized[b] = (teacher_depth[b] - t_min) / (t_max - t_min) * max_depth\n",
    "        else:\n",
    "            # If constant depth, set to 0\n",
    "            normalized[b] = 0.0\n",
    "    return normalized\n",
    "\n",
    "def train_depth_any_audio(teacher, student, train_loader, val_loader, \n",
    "                          optimizer, scheduler, loss_fn, \n",
    "                          num_epochs, device, max_depth=30.0, print_every=5,\n",
    "                          use_feature_kd=False, teacher_model_type='depthanything_v2_vitl',\n",
    "                          grad_accum=1, use_mixed_precision=False):\n",
    "    \"\"\"\n",
    "    Depth Any Audio Training\n",
    "    - With distillation: Teacher generates proxy depth labels from RGB + GT supervision\n",
    "    - With feature KD: Teacher extracts features from spectrogram + GT supervision\n",
    "    - Without distillation: Only GT supervision (supervised learning)\n",
    "    \n",
    "    Teacher depth is normalized to GT scale using min-max normalization\n",
    "    \"\"\"\n",
    "    \n",
    "    use_teacher = teacher is not None\n",
    "    use_feature_level_kd = use_feature_kd and teacher_model_type == 'vitl_feature'\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    if use_teacher:\n",
    "        if use_feature_level_kd:\n",
    "            print(\"ðŸŽµ Starting Depth Any Audio Training (Feature-Level KD)\")\n",
    "            print(\"  âœ“ Teacher: ViT-L (feature extraction from spectrogram)\")\n",
    "            print(\"  âœ“ Student: Audio U-Net (feature-level knowledge distillation)\")\n",
    "        else:\n",
    "            print(\"ðŸŽµ Starting Depth Any Audio Training (Cross-Modal Distillation)\")\n",
    "            print(\"  âœ“ Teacher depth normalization: Enabled (min-max to GT scale)\")\n",
    "    else:\n",
    "        print(\"ðŸ“š Starting Supervised Training (GT only)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_rmse': [], 'val_delta1': []}\n",
    "    \n",
    "    # Mixed precision scaler (fix deprecation warning)\n",
    "    if use_mixed_precision:\n",
    "        try:\n",
    "            scaler = torch.amp.GradScaler('cuda')\n",
    "        except AttributeError:\n",
    "            # Fallback for older PyTorch versions\n",
    "            scaler = torch.cuda.amp.GradScaler()\n",
    "    else:\n",
    "        scaler = None\n",
    "    \n",
    "    if grad_accum > 1:\n",
    "        print(f\"  ðŸ“Š Gradient accumulation: {grad_accum} steps (effective batch size: {DISTILL_BATCH_SIZE * grad_accum})\")\n",
    "    if use_mixed_precision:\n",
    "        print(f\"  âš¡ Mixed precision training: Enabled\")\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # ========== Training ==========\n",
    "        student.train()\n",
    "        if use_teacher:\n",
    "            teacher.eval()\n",
    "        \n",
    "        train_losses = []\n",
    "        optimizer.zero_grad()  # Initialize gradients\n",
    "        \n",
    "        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f'Epoch {epoch}/{num_epochs} [Train]', leave=False)):\n",
    "            images = batch['image'].to(device) if use_teacher else None\n",
    "            audios = batch['audio'].to(device)\n",
    "            depth_gt = batch['depth_gt'].to(device)\n",
    "            \n",
    "            # Step 1: Teacher processing (RGB ì´ë¯¸ì§€ ìž…ë ¥)\n",
    "            # Clear cache before teacher forward\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            if use_teacher:\n",
    "                with torch.no_grad():\n",
    "                    if use_feature_level_kd:\n",
    "                        # Feature-level KD: Teacher extracts features from RGB images\n",
    "                        # Use autocast for teacher too to save memory\n",
    "                        with torch.cuda.amp.autocast(enabled=use_mixed_precision):\n",
    "                            teacher_features = teacher(images, return_features=True)  # Dict of features from RGB\n",
    "                        teacher_depth = None\n",
    "                    else:\n",
    "                        # Prediction-level KD: Teacher generates depth from RGB\n",
    "                        with torch.cuda.amp.autocast(enabled=use_mixed_precision):\n",
    "                            teacher_depth = teacher(images)  # [B, 1, H, W]\n",
    "                        # Normalize teacher depth to GT scale [0, max_depth]\n",
    "                        teacher_depth = normalize_depth_to_gt(teacher_depth, max_depth)\n",
    "                        teacher_features = None\n",
    "            else:\n",
    "                teacher_depth = None\n",
    "                teacher_features = None\n",
    "            \n",
    "            # Step 2: Student predicts depth from audio (ìŠ¤íŽ™íŠ¸ë¡œê·¸ëž¨ ìž…ë ¥)\n",
    "            # Use autocast for mixed precision\n",
    "            with torch.cuda.amp.autocast(enabled=use_mixed_precision):\n",
    "                if use_feature_level_kd:\n",
    "                    # Student also returns features for feature-level KD\n",
    "                    student_depth, student_features = student(audios, return_features=True)\n",
    "                else:\n",
    "                    student_depth = student(audios)  # [B, 1, H, W]\n",
    "                    student_features = None\n",
    "                \n",
    "                # Step 3: Compute loss\n",
    "                if use_feature_level_kd:\n",
    "                    # Feature-level KD: Use teacher and student features\n",
    "                    loss, loss_dict = loss_fn(\n",
    "                        pred=student_depth,\n",
    "                        teacher_target=None,  # No prediction-level KD\n",
    "                        gt_target=depth_gt,\n",
    "                        student_features=student_features,\n",
    "                        teacher_features=teacher_features\n",
    "                    )\n",
    "                else:\n",
    "                    # Prediction-level KD or supervised: Use teacher depth prediction\n",
    "                    loss, loss_dict = loss_fn(\n",
    "                        pred=student_depth,\n",
    "                        teacher_target=teacher_depth,\n",
    "                        gt_target=depth_gt,\n",
    "                        student_features=None,\n",
    "                        teacher_features=None\n",
    "                    )\n",
    "            \n",
    "            # Clear cache more frequently to free memory\n",
    "            if batch_idx % 5 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Delete intermediate variables to free memory\n",
    "            del student_depth\n",
    "            if student_features is not None:\n",
    "                del student_features\n",
    "            if teacher_features is not None:\n",
    "                del teacher_features\n",
    "            if teacher_depth is not None:\n",
    "                del teacher_depth\n",
    "            \n",
    "            # Step 4: Backprop and optimize (with gradient accumulation and mixed precision)\n",
    "            # Scale loss for gradient accumulation\n",
    "            loss = loss / grad_accum\n",
    "            \n",
    "            if use_mixed_precision:\n",
    "                # Mixed precision training\n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                # Update weights every grad_accum steps\n",
    "                if (batch_idx + 1) % grad_accum == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(student.parameters(), max_norm=1.0)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "            else:\n",
    "                # Standard training\n",
    "                loss.backward()\n",
    "                \n",
    "                # Update weights every grad_accum steps\n",
    "                if (batch_idx + 1) % grad_accum == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                    torch.nn.utils.clip_grad_norm_(student.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "            \n",
    "            train_losses.append(loss.item() * grad_accum)  # Scale back for logging\n",
    "        \n",
    "        train_loss = np.mean(train_losses)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        \n",
    "        # ========== Validation ==========\n",
    "        student.eval()\n",
    "        val_losses = []\n",
    "        val_errors = []\n",
    "        \n",
    "        # Clear cache before validation\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f'Epoch {epoch}/{num_epochs} [Val]', leave=False):\n",
    "                images = batch['image'].to(device) if use_teacher else None\n",
    "                audios = batch['audio'].to(device)\n",
    "                depth_gt = batch['depth_gt'].to(device)\n",
    "                \n",
    "                # Teacher processing (RGB ì´ë¯¸ì§€ ìž…ë ¥)\n",
    "                if use_teacher:\n",
    "                    with torch.cuda.amp.autocast(enabled=use_mixed_precision):\n",
    "                        if use_feature_level_kd:\n",
    "                            teacher_features = teacher(images, return_features=True)  # RGBì—ì„œ feature ì¶”ì¶œ\n",
    "                            teacher_depth = None\n",
    "                        else:\n",
    "                            teacher_depth = teacher(images)\n",
    "                            teacher_depth = normalize_depth_to_gt(teacher_depth, max_depth)\n",
    "                            teacher_features = None\n",
    "                else:\n",
    "                    teacher_depth = None\n",
    "                    teacher_features = None\n",
    "                \n",
    "                # Student prediction (Audio ìŠ¤íŽ™íŠ¸ë¡œê·¸ëž¨ ìž…ë ¥)\n",
    "                with torch.cuda.amp.autocast(enabled=use_mixed_precision):\n",
    "                    if use_feature_level_kd:\n",
    "                        student_depth, student_features = student(audios, return_features=True)\n",
    "                    else:\n",
    "                        student_depth = student(audios)\n",
    "                        student_features = None\n",
    "                \n",
    "                # Clear cache periodically during validation\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "                # Compute loss\n",
    "                if use_feature_level_kd:\n",
    "                    loss, _ = loss_fn(\n",
    "                        pred=student_depth,\n",
    "                        teacher_target=None,\n",
    "                        gt_target=depth_gt,\n",
    "                        student_features=student_features,\n",
    "                        teacher_features=teacher_features\n",
    "                    )\n",
    "                else:\n",
    "                    loss, _ = loss_fn(\n",
    "                        pred=student_depth,\n",
    "                        teacher_target=teacher_depth,\n",
    "                        gt_target=depth_gt,\n",
    "                        student_features=None,\n",
    "                        teacher_features=None\n",
    "                    )\n",
    "                val_losses.append(loss.item())\n",
    "                \n",
    "                # Metrics against GT (if available)\n",
    "                if depth_gt.sum() > 0:\n",
    "                    for i in range(student_depth.shape[0]):\n",
    "                        pred = student_depth[i, 0].cpu().numpy()\n",
    "                        gt = depth_gt[i, 0].cpu().numpy()\n",
    "                        \n",
    "                        if gt.max() > 0:\n",
    "                            errors = compute_errors(gt, pred, min_depth_threshold=0.1)\n",
    "                            val_errors.append(errors)\n",
    "        \n",
    "        val_loss = np.mean(val_losses)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        # Compute metrics\n",
    "        if len(val_errors) > 0:\n",
    "            mean_errors = np.array(val_errors).mean(0)\n",
    "            abs_rel, rmse, delta1 = mean_errors[0], mean_errors[1], mean_errors[2]\n",
    "            history['val_rmse'].append(rmse)\n",
    "            history['val_delta1'].append(delta1)\n",
    "        else:\n",
    "            abs_rel, rmse, delta1 = 0, 0, 0\n",
    "        \n",
    "        # Learning rate step\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % print_every == 0 or epoch == 1:\n",
    "            print(f\"\\nEpoch [{epoch}/{num_epochs}]\")\n",
    "            print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"  Val Loss:   {val_loss:.4f}\")\n",
    "            if len(val_errors) > 0:\n",
    "                print(f\"  RMSE:       {rmse:.3f}m\")\n",
    "                print(f\"  ABS_REL:    {abs_rel:.4f}\")\n",
    "                print(f\"  Delta1:     {delta1:.4f}\")\n",
    "            print(f\"  LR:         {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'student_state_dict': student.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "            }, 'best_depth_any_audio.pth')\n",
    "            print(f\"  ðŸŽ¯ Best model saved! (val_loss={val_loss:.4f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ… Training Complete!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸŽµ Starting Depth Any Audio Training (Feature-Level KD)\n",
      "  âœ“ Teacher: ViT-L (feature extraction from spectrogram)\n",
      "  âœ“ Student: Audio U-Net (feature-level knowledge distillation)\n",
      "================================================================================\n",
      "  ðŸ“Š Gradient accumulation: 8 steps (effective batch size: 16)\n",
      "  âš¡ Mixed precision training: Enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 [Train]:   0%|          | 0/956 [00:00<?, ?it/s]/tmp/ipykernel_260734/2896322602.py:96: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_mixed_precision):\n",
      "/tmp/ipykernel_260734/2896322602.py:112: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_mixed_precision):\n",
      "Epoch 1/100 [Val]:   0%|          | 0/313 [00:00<?, ?it/s]            /tmp/ipykernel_260734/2896322602.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_mixed_precision):\n",
      "/tmp/ipykernel_260734/2896322602.py:212: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_mixed_precision):\n",
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/100]\n",
      "  Train Loss: 2.5904\n",
      "  Val Loss:   2.2294\n",
      "  RMSE:       2.710m\n",
      "  ABS_REL:    0.5560\n",
      "  Delta1:     0.3668\n",
      "  LR:         0.000100\n",
      "  ðŸŽ¯ Best model saved! (val_loss=2.2294)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸŽ¯ Best model saved! (val_loss=2.1994)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸŽ¯ Best model saved! (val_loss=2.1026)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [5/100]\n",
      "  Train Loss: 1.8569\n",
      "  Val Loss:   2.0978\n",
      "  RMSE:       2.524m\n",
      "  ABS_REL:    0.5167\n",
      "  Delta1:     0.3935\n",
      "  LR:         0.000099\n",
      "  ðŸŽ¯ Best model saved! (val_loss=2.0978)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸŽ¯ Best model saved! (val_loss=2.0359)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸŽ¯ Best model saved! (val_loss=1.9718)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [10/100]\n",
      "  Train Loss: 1.6072\n",
      "  Val Loss:   2.0033\n",
      "  RMSE:       2.401m\n",
      "  ABS_REL:    0.5154\n",
      "  Delta1:     0.4428\n",
      "  LR:         0.000098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸŽ¯ Best model saved! (val_loss=1.9629)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [15/100]\n",
      "  Train Loss: 1.3849\n",
      "  Val Loss:   1.9585\n",
      "  RMSE:       2.321m\n",
      "  ABS_REL:    0.4465\n",
      "  Delta1:     0.4591\n",
      "  LR:         0.000095\n",
      "  ðŸŽ¯ Best model saved! (val_loss=1.9585)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [20/100]\n",
      "  Train Loss: 1.1966\n",
      "  Val Loss:   2.0264\n",
      "  RMSE:       2.367m\n",
      "  ABS_REL:    0.5258\n",
      "  Delta1:     0.4481\n",
      "  LR:         0.000090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [25/100]\n",
      "  Train Loss: 1.0335\n",
      "  Val Loss:   2.0191\n",
      "  RMSE:       2.317m\n",
      "  ABS_REL:    0.4600\n",
      "  Delta1:     0.4736\n",
      "  LR:         0.000085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100 [Train]:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 857/956 [01:51<00:13,  7.31it/s]"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "\n",
    "history = train_depth_any_audio(\n",
    "    teacher=teacher_model,\n",
    "    student=student_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loss_fn=distill_loss_fn,\n",
    "    num_epochs=DISTILL_EPOCHS,\n",
    "    device=device,\n",
    "    max_depth=MAX_DEPTH,\n",
    "    print_every=PRINT_EVERY if 'PRINT_EVERY' in globals() else 5,\n",
    "    use_feature_kd=USE_FEATURE_KD,\n",
    "    teacher_model_type=TEACHER_MODEL,\n",
    "    grad_accum=DISTILL_GRAD_ACCUM if 'DISTILL_GRAD_ACCUM' in globals() else 1,\n",
    "    use_mixed_precision=USE_MIXED_PRECISION if 'USE_MIXED_PRECISION' in globals() else False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Visualization & Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ–¼ï¸  Visualizing Predictions\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "student_model.eval()\n",
    "teacher_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get a batch\n",
    "    batch = next(iter(val_loader))\n",
    "    images = batch['image'][:4].to(device)\n",
    "    audios = batch['audio'][:4].to(device)\n",
    "    depth_gt = batch['depth_gt'][:4]\n",
    "    \n",
    "    # Teacher prediction (from RGB)\n",
    "    teacher_pred = teacher_model(images).cpu()\n",
    "    \n",
    "    # Student prediction (from Audio)\n",
    "    student_pred = student_model(audios).cpu()\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n",
    "    \n",
    "    for i in range(4):\n",
    "        # RGB Image\n",
    "        axes[i, 0].imshow(images[i].cpu().permute(1, 2, 0))\n",
    "        axes[i, 0].set_title('RGB Input (Teacher)')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Teacher Depth\n",
    "        axes[i, 1].imshow(teacher_pred[i, 0], cmap='magma', vmin=0, vmax=MAX_DEPTH)\n",
    "        axes[i, 1].set_title('Teacher Depth (VFM)')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Student Depth\n",
    "        axes[i, 2].imshow(student_pred[i, 0], cmap='magma', vmin=0, vmax=MAX_DEPTH)\n",
    "        axes[i, 2].set_title('Student Depth (Audio)')\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "        # Ground Truth\n",
    "        axes[i, 3].imshow(depth_gt[i, 0], cmap='magma', vmin=0, vmax=MAX_DEPTH)\n",
    "        axes[i, 3].set_title('Ground Truth')\n",
    "        axes[i, 3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('depth_any_audio_predictions.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nâœ… Predictions saved to: depth_any_audio_predictions.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE\n",
    "if len(history['val_rmse']) > 0:\n",
    "    axes[1].plot(history['val_rmse'], label='RMSE', color='orange', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('RMSE (meters)')\n",
    "    axes[1].set_title('Validation RMSE')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].legend()\n",
    "\n",
    "# Delta1\n",
    "if len(history['val_delta1']) > 0:\n",
    "    axes[2].plot(history['val_delta1'], label='Delta1', color='green', linewidth=2)\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Delta1 (accuracy)')\n",
    "    axes[2].set_title('Validation Delta1')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('depth_any_audio_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Training curves saved to: depth_any_audio_training_curves.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ Summary & Next Steps\n",
    "\n",
    "### ðŸŽ¯ What We Built: **Depth Any Audio**\n",
    "\n",
    "Inspired by [Depth AnyEvent (ICCV 2025)](https://github.com/bartn8/depthanyevent), we implemented cross-modal distillation for audio-based depth estimation:\n",
    "\n",
    "#### Architecture:\n",
    "- **Teacher**: Vision Foundation Model (Depth Anything V2) - processes RGB images\n",
    "- **Student**: Audio U-Net - processes binaural audio spectrograms\n",
    "- **Training**: Teacher generates proxy depth labels â†’ Student learns from audio\n",
    "\n",
    "#### Key Innovation:\n",
    "- **No depth annotations required** during training!\n",
    "- Teacher VFM provides supervision from RGB images\n",
    "- Student learns depth estimation from spatially-aligned binaural audio\n",
    "\n",
    "#### Advantages:\n",
    "1. âœ… Leverages powerful VFMs trained on large-scale image data\n",
    "2. âœ… Eliminates need for expensive depth sensors during training\n",
    "3. âœ… Cross-modal knowledge transfer (vision â†’ audio)\n",
    "4. âœ… Works with BatvisionV2's naturally aligned RGB-Audio-Depth data\n",
    "\n",
    "### ðŸ“Š Expected Results:\n",
    "- **Teacher (RGB)**: Near-perfect depth estimation (VFM)\n",
    "- **Student (Audio)**: Competitive depth estimation without depth supervision\n",
    "- **Gap**: Student < Teacher, but better than random initialization\n",
    "\n",
    "### ðŸš€ Next Steps:\n",
    "1. Train with full dataset (all locations)\n",
    "2. Compare against supervised baseline (GT depth labels)\n",
    "3. Add data augmentation (audio + image)\n",
    "4. Try different teacher models (DINOv2, SAM, etc.)\n",
    "5. Implement recurrent architecture (like Depth AnyEvent's RNN)\n",
    "\n",
    "### ðŸ“š References:\n",
    "- [Depth AnyEvent (ICCV 2025)](https://github.com/bartn8/depthanyevent)\n",
    "- [Depth Anything V2](https://github.com/DepthAnything/Depth-Anything-V2)\n",
    "- [BatVision Dataset (IROS 2023)](https://amandinebtto.github.io/Batvision-Dataset/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evtAnything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
