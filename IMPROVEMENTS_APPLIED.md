# Base + Residual Model - Performance Improvements Applied

## ğŸ¯ ì ìš©ëœ ìˆ˜ì •ì‚¬í•­ ìš”ì•½

ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ë‹¤ìŒ 5ê°€ì§€ í¬ë¦¬í‹°ì»¬í•œ ë¬¸ì œë¥¼ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤:

---

## âœ… 1. Activation Functions ì¶”ê°€ (ê°€ì¥ ì¤‘ìš”!)

### ë¬¸ì œ
- ì¶œë ¥ì— activationì´ ì—†ì–´ì„œ depthê°€ ìŒìˆ˜ê°€ ë  ìˆ˜ ìˆì—ˆìŒ
- Baseì™€ Residualì´ ì œì•½ ì—†ì´ í•™ìŠµ

### í•´ê²°
```python
# Base depth: í•­ìƒ ì–‘ìˆ˜ (êµ¬ì¡°ëŠ” ì–‘ìˆ˜ì—¬ì•¼ í•¨)
base_depth = torch.sigmoid(base_depth_raw) * max_depth

# Residual: +/- í—ˆìš©í•˜ë˜ ì œí•œ (ë³´ì •ê°’, max_depthì˜ Â±20%)
residual = torch.tanh(residual_raw) * (max_depth * 0.2)

# Final: ìœ íš¨ ë²”ìœ„ë¡œ clamp
final_depth = torch.clamp(base_depth + residual, 0, max_depth)
```

**íš¨ê³¼**: Baseê°€ ì‹¤ì œë¡œ ì–‘ìˆ˜ êµ¬ì¡°ë¥¼ í•™ìŠµ, Residualì´ ì‘ì€ ë³´ì •ë§Œ ë‹´ë‹¹

---

## âœ… 2. Gradient Detachment (Curriculum Learning)

### ë¬¸ì œ
- Baseì™€ Residualì´ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµë˜ì§€ ì•ŠìŒ
- ë‘ ë””ì½”ë”ê°€ ê°™ì€ gradientë¥¼ ë°›ìŒ

### í•´ê²°
```python
# Early epochs (1-20): í•¨ê»˜ í•™ìŠµ
if epoch <= warmup_epochs:
    final_depth = base_depth + residual

# Later epochs (21+): Base ê³ ì •, Residualë§Œ í•™ìŠµ
else:
    final_depth = base_depth.detach() + residual
```

**íš¨ê³¼**: 
- Phase 1: Baseê°€ êµ¬ì¡° í•™ìŠµ
- Phase 2: Residualì´ ë””í…Œì¼ refine

---

## âœ… 3. Loss ê°€ì¤‘ì¹˜ ê°œì„ 

### Before
```python
lambda_recon = 1.0
lambda_base = 0.5    # ë„ˆë¬´ ì•½í•¨
lambda_sparse = 0.1  # ë„ˆë¬´ ì•½í•¨
lowpass_kernel = 8   # ë„ˆë¬´ ì‘ìŒ
```

### After
```python
lambda_recon = 1.0
lambda_base = 0.8    # 60% ì¦ê°€
lambda_sparse = 0.2  # 100% ì¦ê°€
lowpass_kernel = 16  # 100% ì¦ê°€
```

**Adaptive Loss (warmup)**:
```python
# Epoch 1-20
lambda_base_init = 1.5   # Base í•™ìŠµ ê°•í™”
lambda_recon_init = 0.5  # Recon ì•½í™”
lambda_sparse = 0.3      # Sparsity ê°•í™”

# Epoch 21+
lambda_base_final = 0.5
lambda_recon_final = 1.0
lambda_sparse = 0.3
```

**íš¨ê³¼**: Baseê°€ ì œëŒ€ë¡œ êµ¬ì¡°ë¥¼ í•™ìŠµí•˜ê³ , Residualì´ ê³¼ë„í•˜ê²Œ ì»¤ì§€ì§€ ì•ŠìŒ

---

## âœ… 4. Low-pass Kernel í¬ê¸° ì¦ê°€

### ë¬¸ì œ
- 8x8 kernelì€ 256x256 ì´ë¯¸ì§€ì— ë„ˆë¬´ ì‘ìŒ (3%)
- êµ¬ì¡° ì¶”ì¶œì´ ë¶ˆì¶©ë¶„

### í•´ê²°
- 8 â†’ 16 (ë‘ ë°° ì¦ê°€)
- 256x256 ì´ë¯¸ì§€ì˜ 6.25% ì˜ì—­

**íš¨ê³¼**: Baseê°€ ë” ë¶€ë“œëŸ¬ìš´ êµ¬ì¡°ë¥¼ í•™ìŠµ

---

## âœ… 5. Max Depth ì œì•½ ì¶”ê°€

### ë¬¸ì œ
- ëª¨ë¸ì´ depth ë²”ìœ„ë¥¼ ëª°ëìŒ

### í•´ê²°
```python
# ëª¨ë¸ ìƒì„± ì‹œ max_depth ì „ë‹¬
model = create_base_residual_model(
    ...
    max_depth=cfg.dataset.max_depth,  # 30.0
    ...
)
```

**íš¨ê³¼**: Sigmoid/Tanhê°€ ì˜¬ë°”ë¥¸ ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§

---

## ğŸ“Š ì˜ˆìƒ íš¨ê³¼

### ì‹œê°í™”ì—ì„œ í™•ì¸í•  ê²ƒ

#### Before (ë¬¸ì œ):
- Base â‰ˆ Final (Residualì´ ê±°ì˜ 0)
- ë˜ëŠ” Residual â‰ˆ Final (Baseê°€ ë¬´ì˜ë¯¸)
- Baseì— high-frequency ë…¸ì´ì¦ˆ

#### After (ê°œì„ ):
- âœ… Base = smooth structure (ë²½, ë°”ë‹¥, ì²œì¥)
- âœ… Residual = small corrections (ë¬¼ì²´ ê²½ê³„, Â±0.2 * max_depth)
- âœ… Final = Base + Residual (ëª…í™•í•œ ë¶„ë¦¬)

### ì„±ëŠ¥ ì§€í‘œ

- **ìˆ˜ë ´ ì†ë„**: 20-30% ë¹¨ë¼ì§
- **RMSE**: 5-10% ê°œì„  ì˜ˆìƒ
- **êµ¬ì¡° ì •í™•ë„**: ë²½ë©´ì´ ë” ì§ì„ ì , ë°”ë‹¥ì´ ë” í‰í‰
- **í•´ì„ì„±**: Base depthë§Œ ë´ë„ ë°© êµ¬ì¡° íŒŒì•… ê°€ëŠ¥

---

## ğŸš€ ìƒˆë¡œìš´ ì‹¤í–‰ ëª…ë ¹ì–´

### ê¸°ë³¸ ì„¤ì • (ê°œì„ ëœ ê¸°ë³¸ê°’)

```bash
python train_base_residual.py \
  --dataset batvisionv2 \
  --batch_size 64 \
  --learning_rate 0.001 \
  --experiment_name improved_v1
```

### Adaptive Loss í¬í•¨ (ê°•ë ¥ ì¶”ì²œ!)

```bash
python train_base_residual.py \
  --dataset batvisionv2 \
  --batch_size 64 \
  --learning_rate 0.001 \
  --use_wandb \
  --use_adaptive_loss \
  --warmup_epochs 20 \
  --experiment_name improved_adaptive
```

### W&Bë¡œ ê¸°ì¡´ ëª¨ë¸ê³¼ ë¹„êµ

```bash
# ìƒˆ ì‹¤í—˜ (ê°œì„ ëœ ë²„ì „)
python train_base_residual.py \
  --dataset batvisionv2 \
  --batch_size 64 \
  --learning_rate 0.001 \
  --use_wandb \
  --use_adaptive_loss \
  --experiment_name v2_improved

# W&Bì—ì„œ bs64_adaptive vs v2_improved ë¹„êµ
```

---

## ğŸ” ê²°ê³¼ í™•ì¸ ë°©ë²•

### 1. ì‹œê°í™” í™•ì¸

```bash
# ìƒˆ ê²°ê³¼ í™•ì¸
ls -lh results/base_residual_*_improved*/epoch_*_decomposition.png

# ì´ë¯¸ì§€ì—ì„œ í™•ì¸:
# - Baseê°€ ë¶€ë“œëŸ¬ìš´ê°€?
# - Residualì´ ì‘ì€ê°€? (blue/redê°€ ì•½í•œê°€?)
# - Finalì´ ì •í™•í•œê°€?
```

### 2. Loss ëª¨ë‹ˆí„°ë§

```python
# í•™ìŠµ ì¤‘ ì¶œë ¥:
Epoch 1: Loss=X.XX (recon=X.XX, base=X.XX, sparse=X.XX)
```

**ê¸°ëŒ€ê°’**:
- `sparse` lossê°€ ì‘ì•„ì•¼ í•¨ (< 0.5)
- `base` lossê°€ ì´ˆê¸°ì— ë¹ ë¥´ê²Œ ê°ì†Œ
- `recon` lossê°€ ì•ˆì •ì ìœ¼ë¡œ ê°ì†Œ

### 3. W&B ëŒ€ì‹œë³´ë“œ

Plot ìƒì„±:
1. **Loss Components**: base vs sparse vs recon over time
2. **Residual Magnitude**: `train/loss_sparse` (ì‘ì•„ì ¸ì•¼ í•¨)
3. **Performance**: `val/rmse` (ê¸°ì¡´ ëª¨ë¸ê³¼ ë¹„êµ)

---

## ğŸ“ ë³€ê²½ëœ íŒŒì¼ë“¤

1. âœ… `models/base_residual_model.py`
   - `__init__`: max_depth íŒŒë¼ë¯¸í„° ì¶”ê°€
   - `forward`: Sigmoid/Tanh activation ì¶”ê°€
   - `create_base_residual_model`: max_depth ì „ë‹¬

2. âœ… `utils_base_residual_loss.py`
   - `BaseResidualLoss`: ê¸°ë³¸ ê°€ì¤‘ì¹˜ ë³€ê²½ (0.8, 0.2, 16)
   - `AdaptiveBaseResidualLoss`: ì´ˆê¸°ê°’ ê°•í™” (1.5, 0.3)

3. âœ… `train_base_residual.py`
   - Argument defaults: ê°œì„ ëœ ê°€ì¤‘ì¹˜
   - Model creation: max_depth ì „ë‹¬
   - Training loop: Gradient detachment ì¶”ê°€
   - Validation loop: ë™ì¼í•œ detachment ì ìš©

---

## ğŸ’¡ ì¶”ê°€ ì‹¤í—˜ ì•„ì´ë””ì–´

### ë” ê³µê²©ì ì¸ ì„¤ì • (í° ë°©, ë³µì¡í•œ êµ¬ì¡°)

```bash
python train_base_residual.py \
  --lambda_base 1.0 \
  --lambda_sparse 0.3 \
  --lowpass_kernel 24 \
  --experiment_name aggressive
```

### ë³´ìˆ˜ì ì¸ ì„¤ì • (ì‘ì€ ë°©, ê°„ë‹¨í•œ êµ¬ì¡°)

```bash
python train_base_residual.py \
  --lambda_base 0.5 \
  --lambda_sparse 0.1 \
  --lowpass_kernel 12 \
  --experiment_name conservative
```

---

## ğŸ“ ì´ë¡ ì  ë°°ê²½

### Taylor Series Analogy

```
f(x) â‰ˆ f(a) + f'(a)(x-a) + ...
      â””â”€ base  â””â”€ residual

Depth(x,y) â‰ˆ Structure(x,y) + Details(x,y)
             â””â”€ sigmoid(base) â””â”€ tanh(res)
```

### Activation ì„ íƒ ì´ìœ 

- **Sigmoid for Base**: 
  - ì¶œë ¥: [0, max_depth]
  - êµ¬ì¡°ëŠ” í•­ìƒ ì–‘ìˆ˜
  - ë¶€ë“œëŸ¬ìš´ gradient

- **Tanh for Residual**:
  - ì¶œë ¥: [-0.2*max_depth, +0.2*max_depth]
  - ë³´ì •ì€ +/- í—ˆìš©
  - ì¤‘ì‹¬ì´ 0 (ê¸°ë³¸ì ìœ¼ë¡œ ë³´ì • ì•ˆ í•¨)

---

## âœ… ì„±ê³µ ì§€í‘œ

ë‹¤ìŒì„ ë‹¬ì„±í•˜ë©´ ì„±ê³µ:

1. âœ… Base depthê°€ ë¶€ë“œëŸ½ê³  ì–‘ìˆ˜
2. âœ… Residualì´ ì‘ìŒ (í‰ê·  ì ˆëŒ“ê°’ < 1.0)
3. âœ… Final RMSEê°€ ê¸°ì¡´ ëŒ€ë¹„ 5% ì´ìƒ ê°œì„ 
4. âœ… ìˆ˜ë ´ì´ 20 epoch ì´ë‚´ì— ì•ˆì •í™”
5. âœ… Visualizationì—ì„œ ëª…í™•í•œ ë¶„ë¦¬

---

**ëª¨ë“  ìˆ˜ì •ì‚¬í•­ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤! ğŸ‰**

ì§€ê¸ˆ ë°”ë¡œ í•™ìŠµì„ ì‹œì‘í•˜ì„¸ìš”:

```bash
python train_base_residual.py \
  --dataset batvisionv2 \
  --batch_size 64 \
  --learning_rate 0.001 \
  --use_wandb \
  --use_adaptive_loss \
  --experiment_name improved_final
```

